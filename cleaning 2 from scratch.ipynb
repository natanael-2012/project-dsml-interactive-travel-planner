{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# from google.colab import userdata\n",
    "# HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "# userdata.get('OPENAI_API_KEY')\n",
    "\n",
    "load_dotenv()\n",
    "# huggingface_token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "# from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Mount Google Drive ==========\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# ========== Path Configuration ==========\n",
    "# Update these paths according to your Google Drive structure\n",
    "DRIVE_BASE = '/content/drive/MyDrive/'\n",
    "ZIP_PATH = \"./data/elmundo_chunked_es_page1_40years.zip\"\n",
    "EXTRACT_DIR = '/tmp/extracted'  # Using tmp for faster I/O\n",
    "OUTPUT_DIR = os.path.join(DRIVE_BASE, 'cleaned_articles1')\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(EXTRACT_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZIP_PATH = \"./data/elmundo_chunked_es_page1_40years.zip\"\n",
    "EXTRACT_DIR = '/tmp/extracted'  # Using tmp for faster I/O\n",
    "os.makedirs(EXTRACT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== File Extraction ==========\n",
    "def extract_files():\n",
    "    with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
    "        # Extract nested structure\n",
    "        for file in zip_ref.namelist():\n",
    "            if file.endswith('.txt'):\n",
    "                zip_ref.extract(file, EXTRACT_DIR)\n",
    "    print(\"*\" * 50)\n",
    "    print(f\"Extracted files to: {EXTRACT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Extract zip file\n",
    "2. open the folder\n",
    "3. For each file in folder, \n",
    "    read the content and extract the text\n",
    "    <!-- chunk the text into 1000 words -->\n",
    "    <!-- pass chunks to the model so it can fix the spelling -->\n",
    "    translate the corrected text to english\n",
    "    <!-- add the file name and the corrected text to a dictionary -->\n",
    "    save the corrected text to a new file\n",
    "4. Save the dictionary to a pkl file\n",
    "5. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    api_key=OPENAI_API_KEY\n",
    ")\n",
    "\n",
    "def correct_with_openai(text, filename, just_text = True, max_completion_tokens = 2048, temperature = 1, top_p = 1, frequency_penalty=0, presence_penalty=0,**kwargs):\n",
    "  response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "          {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": f\"Eres un experto en documentos históricos de Puerto Rico. El texto en español son noticias del siglo XX y contiene muchos errores a causa del OCR. Descifra el contenido y tradúcelo al inglés:\\n1. Preserva nombres propios (ej: Mayagüez, Caguas)\\n2. Ignora el \\\"header\\\" (ej:\\n```EL MUNDO\\nPRONOSTICOS DEL TIEMPO PARA LA ISLA, HOY: Mayormente nublado, con aguaceros dispersos temprano en la mafiana. EN SAN JUAN. AYER: Temperatura máxima. 80; mínima, 77. Presión barométrica al nivel del mar, a las 4:80 de la tarde. 38.88 pulgadas de mercurio. No hay indicios de disturbio tropical.\\n40 páginas 5/\\nDIARIO DE LA MARANA\\nAÑO XXVIII\\nEntered aa second clsss matter, Post Office, San Juan, P. R.)```\\n3. Ignora los anuncios\\n4. Solo mantén contenido relacionado a Puerto Rico (especialmente sobre ciudades, locaciones o eventos históricos)\\n5. Traduce el texto a inglés. Solo mantén los datos mas importantes\\n6.  Lista las ciudades o locaciones de Puerto Rico mencionadas\\n7. Escribe solo en texto (no uses **negrillas** ni *itálicas* ni nada en markdown)\\n8. return it as a JSON object with two fields:\\n    - 'metadata': un diccionario con la siguiente informacion: 'filename' (nombre del articulo), 'date' (fecha del articulo), 'locations' (lista de las ciudades o locaciones de Puerto Rico mencionadas).\\n    - 'text': the corrected and summarized text in English.\\n8. No digas nada mas ni preguntes más. El nombre del articulo es {filename}. Usa el siguiente texto: {text}\"\n",
    "          }\n",
    "        ]\n",
    "      }\n",
    "    ],\n",
    "    response_format={\n",
    "      \"type\": \"json_object\"\n",
    "    },\n",
    "    temperature=temperature,\n",
    "    max_completion_tokens=max_completion_tokens,\n",
    "    top_p=top_p,\n",
    "    frequency_penalty=frequency_penalty,\n",
    "    presence_penalty=presence_penalty,\n",
    "    **kwargs\n",
    "  )\n",
    "  if just_text:\n",
    "    return response.choices[0].message.content\n",
    "  \n",
    "  return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pickle as pkl\n",
    "\n",
    "def save_progress(data, filename=\"all_docs.pkl\"):\n",
    "    \"\"\" Save the current state of data to Google Drive. \"\"\"\n",
    "    save_path = os.path.join(OUTPUT_DIR, filename)\n",
    "    \n",
    "    with open(save_path, 'wb') as f:\n",
    "        pkl.dump(data, f)\n",
    "    \n",
    "    print(f\"Progress saved at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROGRESS_FILE = os.path.join(OUTPUT_DIR, \"processed_files.log\")\n",
    "\n",
    "def get_processed_files():\n",
    "    if os.path.exists(PROGRESS_FILE):\n",
    "        with open(PROGRESS_FILE, 'r') as f:\n",
    "            return set(f.read().splitlines())\n",
    "    return set()\n",
    "\n",
    "def update_progress(filename):\n",
    "    with open(PROGRESS_FILE, 'a') as f:\n",
    "        f.write(f\"{filename}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Processing Pipeline ==========\n",
    "import json\n",
    "import pickle as pkl\n",
    "from langchain.docstore.document import Document\n",
    "import time\n",
    "\n",
    "# Save progress every 30 minutes\n",
    "interval_minutes = 15\n",
    "\n",
    "def process_files():\n",
    "    extract_files()\n",
    "\n",
    "    all_docs = []\n",
    "\n",
    "    # Track when the last save occurred\n",
    "    last_save_time = time.time()\n",
    "    processed = get_processed_files()\n",
    "\n",
    "    # Get all text files from nested directory\n",
    "    base_dir = os.path.join(EXTRACT_DIR, \"elmundo_chunked_es_page1_40years\")\n",
    "    txt_files = [f for f in os.listdir(base_dir) if f.endswith('.txt')]\n",
    "\n",
    "    for filename in tqdm(txt_files, desc=\"Processing files\"):\n",
    "\n",
    "        if filename in processed:\n",
    "            # Skip already processed files\n",
    "            continue\n",
    "\n",
    "        input_path = os.path.join(base_dir, filename)\n",
    "        output_path = os.path.join(OUTPUT_DIR, f\"cleaned_{filename}\")\n",
    "\n",
    "        with open(input_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            raw_text = f.read()\n",
    "\n",
    "        try:\n",
    "            # cleaned text is a JSON object with 'metadata' and 'text' fields:\n",
    "            json_object = json.loads(correct_with_openai(raw_text, filename))  # OpenAI version\n",
    "            cleaned_text = json_object.get('text')\n",
    "\n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(cleaned_text.get('text'))\n",
    "\n",
    "            print(f\"Processed: {filename} -> Saved to Drive\")\n",
    "\n",
    "            doc = Document(\n",
    "                page_content=json_object['text'],\n",
    "                metadata=json_object['metadata']\n",
    "            )\n",
    "            all_docs.append(doc)\n",
    "            \n",
    "            # Update the processed log\n",
    "            update_progress(filename)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "        current_time = time.time()\n",
    "        if (current_time - last_save_time) >= (interval_minutes * 60):\n",
    "            save_progress(all_docs)\n",
    "            last_save_time = current_time  # Update the last save time\n",
    "\n",
    "    # Save all_docs as pkl file\n",
    "    with open(os.path.join(OUTPUT_DIR, \"all_docs.pkl\"), 'wb') as f:\n",
    "        pkl.dump(all_docs, f)\n",
    "\n",
    "    with open(\"all_docs.pkl\", 'wb') as f:\n",
    "        pkl.dump(all_docs, f)\n",
    "\n",
    "    return all_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = process_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ========== Chroma =========="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
