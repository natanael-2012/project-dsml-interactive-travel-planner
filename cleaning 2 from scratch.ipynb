{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "3sx2JaTzBPzh"
      },
      "outputs": [],
      "source": [
        "# importing the required libraries. Some libraries run on colab while others run locally\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "from tqdm import tqdm\n",
        "from openai import OpenAI\n",
        "\n",
        "try:\n",
        "    from dotenv import load_dotenv\n",
        "    load_dotenv()\n",
        "    HF_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
        "    OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "    \n",
        "except:\n",
        "    from google.colab import drive\n",
        "    from google.colab import userdata\n",
        "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Set to True to run the pipelines. Since we stored the results in a file, \n",
        "# we can set this to False to avoid running the pipelines again\n",
        "RUN_PIPELINES = False   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "LJcTvPEABPzk"
      },
      "outputs": [],
      "source": [
        "# from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WBsR5kABPzm",
        "outputId": "353d93b0-67ec-4064-c166-064e5799f941"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# same as above. If on colab, we need to mount the drive. If locally, we can set the path to the local directory\n",
        "\n",
        "# ========== Mount Google Drive ==========\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# ========== Path Configuration ==========\n",
        "# Update these paths according to your Google Drive structure\n",
        "    DRIVE_BASE = '/content/drive/MyDrive/'\n",
        "    EXTRACT_DIR = '/tmp/extracted'  # Using tmp for faster I/O\n",
        "\n",
        "exept:\n",
        "    DRIVE_BASE = './outputs/'\n",
        "    EXTRACT_DIR = './tmp/extracted'  # Using tmp for faster I/O\n",
        "\n",
        "ZIP_PATH = \"./data/elmundo_chunked_es_page1_40years.zip\"\n",
        "OUTPUT_DIR = os.path.join(DRIVE_BASE, 'cleaned_articles1')\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(EXTRACT_DIR, exist_ok=True)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xl-0upqUBPzq"
      },
      "outputs": [],
      "source": [
        "# ========== File Extraction ==========\n",
        "def extract_files(zip_path, extract_dir):\n",
        "    \"\"\"\n",
        "    Extracts files from a zip archive to a directory.\n",
        "    \n",
        "    Args:\n",
        "    zip_path (str): Path to the zip archive.\n",
        "    extract_dir (str): Directory to extract the files to.\n",
        "    \"\"\"\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        # Extract nested structure\n",
        "        for file in zip_ref.namelist():\n",
        "            if file.endswith('.txt'):\n",
        "                zip_ref.extract(file, extract_dir)\n",
        "    print(\"*\" * 50)\n",
        "    print(f\"Extracted files to: {extract_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrYai7AvBPzq"
      },
      "source": [
        "1. Extract zip file\n",
        "2. open the folder\n",
        "3. For each file in folder,\n",
        "    read the content and extract the text\n",
        "    <!-- chunk the text into 1000 words -->\n",
        "    <!-- pass chunks to the model so it can fix the spelling -->\n",
        "    translate the corrected text to english\n",
        "    <!-- add the file name and the corrected text to a dictionary -->\n",
        "    save the corrected text to a new file\n",
        "4. Save the dictionary to a pkl file\n",
        "5.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "76WAjMOFBPzs"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(\n",
        "    api_key=OPENAI_API_KEY\n",
        ")\n",
        "\n",
        "def correct_with_openai(text, filename, just_text = True, max_completion_tokens = 2048, temperature = 1, top_p = 1, frequency_penalty=0, presence_penalty=0,**kwargs):\n",
        "  \"\"\"\n",
        "  Corrects text using OpenAI's GPT-4o-mini model.\n",
        "  \n",
        "  Args:\n",
        "  text (str): The text to correct.\n",
        "  filename (str): The name of the file.\n",
        "  just_text (bool): Whether to return just the corrected text.\n",
        "  max_completion_tokens (int): The maximum number of tokens to generate.\n",
        "  temperature (float): The temperature for sampling.\n",
        "  top_p (float): The nucleus sampling probability.\n",
        "  frequency_penalty (float): The frequency penalty.\n",
        "  presence_penalty (float): The presence penalty.\n",
        "  kwargs: Additional keyword arguments.\n",
        "  \n",
        "  Returns:\n",
        "  str: The corrected text.\n",
        "  \"\"\"\n",
        "  response = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[\n",
        "      {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "          {\n",
        "            \"type\": \"text\",\n",
        "            \"text\": f\"Eres un experto en documentos históricos de Puerto Rico. El texto en español son noticias del siglo XX y contiene muchos errores a causa del OCR. Descifra el contenido y tradúcelo al inglés:\\n1. Preserva nombres propios (ej: Mayagüez, Caguas)\\n2. Ignora el \\\"header\\\" (ej:\\n```EL MUNDO\\nPRONOSTICOS DEL TIEMPO PARA LA ISLA, HOY: Mayormente nublado, con aguaceros dispersos temprano en la mafiana. EN SAN JUAN. AYER: Temperatura máxima. 80; mínima, 77. Presión barométrica al nivel del mar, a las 4:80 de la tarde. 38.88 pulgadas de mercurio. No hay indicios de disturbio tropical.\\n40 páginas 5/\\nDIARIO DE LA MARANA\\nAÑO XXVIII\\nEntered aa second clsss matter, Post Office, San Juan, P. R.)```\\n3. Ignora los anuncios\\n4. Solo mantén contenido relacionado a Puerto Rico (especialmente sobre ciudades, locaciones o eventos históricos)\\n5. Traduce el texto a inglés. Solo mantén los datos mas importantes\\n6.  Lista las ciudades o locaciones de Puerto Rico mencionadas\\n7. Escribe solo en texto (no uses **negrillas** ni *itálicas* ni nada en markdown)\\n8. return it as a JSON object with two fields:\\n    - 'metadata': un diccionario con la siguiente informacion: 'filename' (nombre del articulo), 'date' (fecha del articulo), 'locations' (lista de las ciudades o locaciones de Puerto Rico mencionadas).\\n    - 'text': the corrected and summarized text in English.\\n8. No digas nada mas ni preguntes más. El nombre del articulo es {filename}. Usa el siguiente texto: {text}\"\n",
        "          }\n",
        "        ]\n",
        "      }\n",
        "    ],\n",
        "    response_format={\n",
        "      \"type\": \"json_object\"\n",
        "    },\n",
        "    temperature=temperature,\n",
        "    max_completion_tokens=max_completion_tokens,\n",
        "    top_p=top_p,\n",
        "    frequency_penalty=frequency_penalty,\n",
        "    presence_penalty=presence_penalty,\n",
        "    **kwargs\n",
        "  )\n",
        "  if just_text:\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "  return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "jRwc-LuNBPzt"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "import pickle as pkl\n",
        "\n",
        "def save_progress(data, filename=\"all_docs.pkl\"):\n",
        "    \"\"\" Save the current state of data to Google Drive. \n",
        "    \n",
        "    Args:\n",
        "    data (dict): The data to save.\n",
        "    filename (str): The filename to save the data to.\n",
        "    \n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    save_path = os.path.join(OUTPUT_DIR, filename)\n",
        "\n",
        "    with open(save_path, 'wb') as f:\n",
        "        pkl.dump(data, f)\n",
        "\n",
        "    print(f\"Progress saved at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} to {save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "sgLrBamrBPzu"
      },
      "outputs": [],
      "source": [
        "PROGRESS_FILE = os.path.join(OUTPUT_DIR, \"processed_files.log\")\n",
        "\n",
        "def get_processed_files():\n",
        "    \"\"\"\n",
        "    Returns a set of processed files.\n",
        "    \n",
        "    Returns:\n",
        "    set: The set of processed files\n",
        "    \"\"\"\n",
        "    if os.path.exists(PROGRESS_FILE):\n",
        "        with open(PROGRESS_FILE, 'r') as f:\n",
        "            return set(f.read().splitlines())\n",
        "    return set()\n",
        "\n",
        "def update_progress(filename):\n",
        "    \"\"\"\n",
        "    Updates the progress file with the processed filename.\n",
        "    \n",
        "    Args:\n",
        "    filename (str): The filename to add to the progress file.\n",
        "    \n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    with open(PROGRESS_FILE, 'a') as f:\n",
        "        f.write(f\"{filename}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "55H29AJ_BPzv"
      },
      "outputs": [],
      "source": [
        "# ========== Processing Pipeline ==========\n",
        "import json\n",
        "import pickle as pkl\n",
        "from langchain.docstore.document import Document\n",
        "import time\n",
        "\n",
        "# Save progress every 15 minutes\n",
        "interval_minutes = 15\n",
        "\n",
        "def process_files():\n",
        "    \"\"\"\n",
        "    Processes the text files in the ZIP archive.\n",
        "    \n",
        "    Returns:\n",
        "    list: A list of Document objects.\n",
        "    \"\"\"\n",
        "    extract_files(ZIP_PATH, EXTRACT_DIR)  # extract files from zip\n",
        "\n",
        "    all_docs = [] # for storing all the documents\n",
        "\n",
        "    # Track when the last save occurred\n",
        "    last_save_time = time.time()\n",
        "    processed = get_processed_files()\n",
        "\n",
        "    # Get all text files from nested directory\n",
        "    base_dir = os.path.join(EXTRACT_DIR, \"elmundo_chunked_es_page1_40years\")\n",
        "    txt_files = [f for f in os.listdir(base_dir) if f.endswith('.txt')]\n",
        "\n",
        "    for filename in tqdm(txt_files, desc=\"Processing files\"):\n",
        "\n",
        "        if filename in processed:\n",
        "            # Skip already processed files\n",
        "            continue\n",
        "\n",
        "        input_path = os.path.join(base_dir, filename)\n",
        "        output_path = os.path.join(OUTPUT_DIR, f\"cleaned_{filename}\")\n",
        "\n",
        "        with open(input_path, 'r', encoding='utf-8', errors='ignore') as f: # open current text file\n",
        "            raw_text = f.read()\n",
        "\n",
        "        try:\n",
        "            # gets gpt-4o-mini JSON object with 'metadata' and 'text' fields:\n",
        "            json_object = json.loads(correct_with_openai(raw_text, filename))  # OpenAI version\n",
        "\n",
        "            cleaned_text = json_object['text']  # get the text from the gpt-4o-mini model\n",
        "\n",
        "            with open(output_path, 'w', encoding='utf-8') as f: # save text on google drive\n",
        "                f.write(cleaned_text)\n",
        "\n",
        "            print(f\"Processed: {filename} -> Saved to Drive\")\n",
        "\n",
        "            doc = Document(                           # convert text to a langchain text object (for use on Chroma later)\n",
        "                page_content=json_object['text'],\n",
        "                metadata=json_object['metadata']\n",
        "            )\n",
        "            all_docs.append(doc)                      # append docs to list\n",
        "\n",
        "            # Update the processed log\n",
        "            update_progress(filename)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {filename}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "        current_time = time.time()\n",
        "        if (current_time - last_save_time) >= (interval_minutes * 60):\n",
        "            save_progress(all_docs)\n",
        "            last_save_time = current_time  # Update the last save time\n",
        "\n",
        "    # Save all_docs as pkl file\n",
        "    with open(os.path.join(OUTPUT_DIR, \"all_docs.pkl\"), 'wb') as f:\n",
        "        pkl.dump(all_docs, f)\n",
        "\n",
        "    with open(\"all_docs.pkl\", 'wb') as f:\n",
        "        pkl.dump(all_docs, f)\n",
        "\n",
        "    return all_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4MMyhLCBPzw",
        "outputId": "44f2cff0-b61b-4680-a45b-10625a937ea8"
      },
      "outputs": [],
      "source": [
        "# Run the pipeline. False by default, set to True to run in the first cell\n",
        "if RUN_PIPELINES:\n",
        "    all_docs = process_files()\n",
        "else:\n",
        "    with open(\"all_docs.pkl\", 'rb') as f:\n",
        "        all_docs = pkl.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_uLBiuOyBPzx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [],
      "source": [
        "def translate_list (lista, just_text = True, max_completion_tokens = 2048, temperature = 1, top_p = 1, frequency_penalty=0, presence_penalty=0,**kwargs):\n",
        "  \"\"\"\n",
        "  Translates a list of items to English using OpenAI's GPT-4o-mini model.\n",
        "  \n",
        "  Args:\n",
        "  lista (list): The list to translate.\n",
        "  just_text (bool): Whether to return just the translated text.\n",
        "  \n",
        "  Returns:\n",
        "  str: The translated text in a JSON-ish style.\n",
        "  \"\"\"\n",
        "  largo = len(lista)\n",
        "\n",
        "  if isinstance(lista, list):\n",
        "    lista = str(lista)\n",
        "\n",
        "  response = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[\n",
        "      {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "          {\n",
        "            \"type\": \"text\",\n",
        "            \"text\": f\"\"\"\n",
        "                Eres un experto en documentos históricos, localidades, y personas ilustres de Puerto Rico. La lista que se te pasará es un de lugares conocidos y \n",
        "                personas ilustres de Puerto Rico. Tu deber es traducir la lista al inglés y corregir cualquier error que encuentres. \n",
        "                1. Preserva nombres propios (ej: Mayagüez, Caguas, Julia de Burgos)\\n\n",
        "                2. Escribe solo en texto (no uses **negrillas** ni *itálicas* ni nada en markdown)\\n\n",
        "                3. Es posible que la lista ya contenga elementos en inglés. En ese caso, no los traduzcas, pero incluyelos en la respuesta final.\\n\n",
        "                3. Retorna un objeto JSON con {largo} pares key-value:\\n\n",
        "                    - key: el texto de la lista. Podría estar en inglés o español. Depende de como te lo dieron en la lista\\n\n",
        "                    - value: el texto traducido al inglés. Si el elemento ya estaba en inglés, no hace falta traducirlo, pero igualmente incluyes el texto aqui\\n\n",
        "                4. El ejemplo de como se veria la respuesta JSON aceptable para la lista de ejemplo [\"playa de Camuy\", \"Julia de Burgos\", \"Parque acuatico Las Cascadas\", \"Aguada Transmission Center\", \"Domes Beach\"]:\\n\n",
        "                    ```\n",
        "                    {{\n",
        "                        \"playa de Camuy\": \"Camuy Beach\",\n",
        "                        \"Julia de Burgos\": \"Julia de Burgos\",\n",
        "                        \"Parque acuático Las Cascadas\": \"Las Cascadas Water Park\",\n",
        "                        \"Centro Ceremonial Indígena de Caguana\": \"Caguana Indigenous Ceremonial Center\",\n",
        "                        \"Aguada Transmission Center\": \"Aguada Transmission Center\",\n",
        "                        \"Domes beach\": \"Domes beach\"\n",
        "                    }}\n",
        "                    ```\n",
        "                  En general, el JSON deberia verse {{key_1: value_1, key_2: value_2, key_3: value_3, ..., key_{largo}: value_{largo}}}\\n\n",
        "                5. La lista de arriba solamente es un ejemplo para que te guies.\n",
        "                6. Absolutamente todos los elementos en la que el usuario te de tienen que aparecer en el JSON final con su traducción correspondiente.\n",
        "                7. No digas nada mas ni preguntes más.\n",
        "                8. La lista del usuario que vas a usar para el JSON es la siguiente:\\n \n",
        "                    {lista}\n",
        "                \"\"\"\n",
        "          }\n",
        "        ]\n",
        "      }\n",
        "    ],\n",
        "    response_format={\n",
        "      \"type\": \"json_object\"\n",
        "    },\n",
        "    temperature=temperature,\n",
        "    max_completion_tokens=max_completion_tokens,\n",
        "    top_p=top_p,\n",
        "    frequency_penalty=frequency_penalty,\n",
        "    presence_penalty=presence_penalty,\n",
        "    **kwargs\n",
        "  )\n",
        "  if just_text:\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "  return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "def update_metadata_with_landmarks(all_docs, landmarks_dict):\n",
        "    \"\"\"\n",
        "    Updates the metadata of the documents with the landmarks found in the text.\n",
        "    \n",
        "    Args:\n",
        "    all_docs (list): A list of Document objects.\n",
        "    landmarks_dict (dict): A dictionary of landmarks in English and Spanish.\n",
        "    \n",
        "    Returns:\n",
        "    list: A list of Document objects with updated metadata.\n",
        "    \"\"\"\n",
        "    for doc in all_docs:\n",
        "        text = doc.page_content.lower()  # Convert to lowercase for easier matching\n",
        "        for landmark_es, landmark_en in landmarks_dict.items():\n",
        "            if landmark_es.lower() in text or landmark_en.lower() in text:\n",
        "                if 'locations' not in doc.metadata:\n",
        "                    doc.metadata['locations'] = []\n",
        "                if landmark_en not in doc.metadata['locations']:\n",
        "                    doc.metadata['locations'].append(landmark_en)  # Add the English landmark\n",
        "                if landmark_es not in doc.metadata['locations']:\n",
        "                    doc.metadata['locations'].append(landmark_es)  # Add the Spanish landmark\n",
        "\n",
        "    return all_docs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
            "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
            "      --------------------------------------- 0.3/12.8 MB ? eta -:--:--\n",
            "     - ------------------------------------- 0.5/12.8 MB 932.9 kB/s eta 0:00:14\n",
            "     --- ------------------------------------ 1.0/12.8 MB 1.4 MB/s eta 0:00:09\n",
            "     ---- ----------------------------------- 1.3/12.8 MB 1.4 MB/s eta 0:00:09\n",
            "     ----- ---------------------------------- 1.8/12.8 MB 1.6 MB/s eta 0:00:07\n",
            "     ------ --------------------------------- 2.1/12.8 MB 1.6 MB/s eta 0:00:07\n",
            "     ------- -------------------------------- 2.4/12.8 MB 1.6 MB/s eta 0:00:07\n",
            "     --------- ------------------------------ 2.9/12.8 MB 1.6 MB/s eta 0:00:07\n",
            "     --------- ------------------------------ 3.1/12.8 MB 1.6 MB/s eta 0:00:06\n",
            "     ----------- ---------------------------- 3.7/12.8 MB 1.7 MB/s eta 0:00:06\n",
            "     ------------ --------------------------- 3.9/12.8 MB 1.6 MB/s eta 0:00:06\n",
            "     ------------- -------------------------- 4.5/12.8 MB 1.7 MB/s eta 0:00:05\n",
            "     -------------- ------------------------- 4.7/12.8 MB 1.7 MB/s eta 0:00:05\n",
            "     ---------------- ----------------------- 5.2/12.8 MB 1.7 MB/s eta 0:00:05\n",
            "     ----------------- ---------------------- 5.5/12.8 MB 1.7 MB/s eta 0:00:05\n",
            "     ------------------ --------------------- 6.0/12.8 MB 1.7 MB/s eta 0:00:04\n",
            "     ------------------- -------------------- 6.3/12.8 MB 1.7 MB/s eta 0:00:04\n",
            "     --------------------- ------------------ 6.8/12.8 MB 1.7 MB/s eta 0:00:04\n",
            "     ---------------------- ----------------- 7.1/12.8 MB 1.7 MB/s eta 0:00:04\n",
            "     ----------------------- ---------------- 7.6/12.8 MB 1.7 MB/s eta 0:00:04\n",
            "     ------------------------ --------------- 7.9/12.8 MB 1.7 MB/s eta 0:00:03\n",
            "     -------------------------- ------------- 8.4/12.8 MB 1.7 MB/s eta 0:00:03\n",
            "     --------------------------- ------------ 8.7/12.8 MB 1.7 MB/s eta 0:00:03\n",
            "     ---------------------------- ----------- 9.2/12.8 MB 1.7 MB/s eta 0:00:03\n",
            "     ----------------------------- ---------- 9.4/12.8 MB 1.7 MB/s eta 0:00:02\n",
            "     ------------------------------ --------- 9.7/12.8 MB 1.7 MB/s eta 0:00:02\n",
            "     ------------------------------- -------- 10.0/12.8 MB 1.7 MB/s eta 0:00:02\n",
            "     -------------------------------- ------- 10.5/12.8 MB 1.7 MB/s eta 0:00:02\n",
            "     --------------------------------- ------ 10.7/12.8 MB 1.7 MB/s eta 0:00:02\n",
            "     ----------------------------------- ---- 11.3/12.8 MB 1.7 MB/s eta 0:00:01\n",
            "     ------------------------------------ --- 11.5/12.8 MB 1.7 MB/s eta 0:00:01\n",
            "     ------------------------------------- -- 12.1/12.8 MB 1.7 MB/s eta 0:00:01\n",
            "     -------------------------------------- - 12.3/12.8 MB 1.7 MB/s eta 0:00:01\n",
            "     ---------------------------------------  12.6/12.8 MB 1.7 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 12.8/12.8 MB 1.7 MB/s eta 0:00:00\n",
            "Installing collected packages: en-core-web-sm\n",
            "Successfully installed en-core-web-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "# !pip install spacy\n",
        "!spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {},
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "# Load spaCy's NER model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Example function to process the documents and add NER results to metadata\n",
        "def enrich_metadata_with_ner(all_docs):\n",
        "    \"\"\"\n",
        "    Enriches the metadata of the documents with named entities recognized by spaCy.\n",
        "    \n",
        "    Args:\n",
        "    all_docs (list): A list of Document objects.\n",
        "    \n",
        "    Returns:\n",
        "    list: A list of Document objects with updated metadata.\n",
        "    \"\"\"\n",
        "    for doc in all_docs:\n",
        "        text = doc.page_content\n",
        "        spacy_doc = nlp(text)  # Process text through spaCy NER engine\n",
        "\n",
        "        # Collect the detected locations (GPE and LOC entities)\n",
        "        ner_locations = {ent.text for ent in spacy_doc.ents if ent.label_ in ['GPE', 'LOC']}\n",
        "        \n",
        "        # Combine with existing locations in metadata\n",
        "        existing_locations = set(doc.metadata.get('locations', []))\n",
        "        updated_locations = list(existing_locations.union(ner_locations))\n",
        "        \n",
        "        # Update the document's metadata with enriched locations\n",
        "        doc.metadata['locations'] = updated_locations\n",
        "    \n",
        "    return all_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_landmarks_dict():\n",
        "    \"\"\"\n",
        "    Returns a dictionary of landmarks in English and Spanish.\n",
        "    \n",
        "    Returns:\n",
        "    dict: A dictionary of landmarks in English and Spanish.\n",
        "    \"\"\"\n",
        "\n",
        "    path_zip = \"./data/landmarks.zip\"\n",
        "    extract = './tmp/extracted_landmarks'  # Using tmp for faster I/O\n",
        "    # Create directories\n",
        "    os.makedirs(extract, exist_ok=True)\n",
        "\n",
        "    # Extract the landmarks.zip file\n",
        "    extract_files(path_zip, extract)\n",
        "\n",
        "\n",
        "    #landmarks list of file names, removing .txt and changing `_`, and `-` to spaces\n",
        "    base_dir = os.path.join(extract, \"landmarks\")\n",
        "    landmarks = [f.replace('.txt', '').replace('_', ' ').replace('-', ' ') for f in os.listdir(base_dir) if f.endswith('.txt')]\n",
        "\n",
        "    # Translate the landmarks list with OpenAI (splitting into two to avoid exceeding the token limit)\n",
        "    translations1 = translate_list(landmarks[:len(landmarks)//2], just_text=False, max_completion_tokens=7000)\n",
        "    translations2 = translate_list(landmarks[len(landmarks)//2:], just_text=False, max_completion_tokens=7000)\n",
        "\n",
        "    # Combine the translations. translations contain 2 ChatCompletion objects\n",
        "    translations = [translations1, translations2]\n",
        "\n",
        "    #save translations to pkl\n",
        "    with open(\"landmark translations.pkl\", 'wb') as f:\n",
        "        pkl.dump(translations, f)\n",
        "\n",
        "    # change the translations to json\n",
        "    translation1_json = json.loads(translations[0].choices[0].message.content)\n",
        "    translation2_json = json.loads(translations[1].choices[0].message.content)\n",
        "\n",
        "    # make translations_json by adding translation1_json and translation2_json\n",
        "    translations_json = {**translation1_json, **translation2_json}\n",
        "\n",
        "    # save translations_json to json file\n",
        "    with open(\"landmarks.json\", 'w') as f:\n",
        "        json.dump(translations_json, f)\n",
        "\n",
        "    return translations_json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {},
      "outputs": [],
      "source": [
        "# delete the extracted files\n",
        "! rm -rf ./tmp/extracted_landmarks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the pipeline. False by default, set to True to run in the first cell\n",
        "if RUN_PIPELINES:\n",
        "    translations = get_landmarks_dict()\n",
        "    # update metadata with landmarks\n",
        "    all_docs = update_metadata_with_landmarks(all_docs, translations)\n",
        "    all_docs = enrich_metadata_with_ner(all_docs)\n",
        "    # save the updated all_docs to pkl\n",
        "    with open(\"all_docs_updated.pkl\", 'wb') as f:\n",
        "        pkl.dump(all_docs, f)\n",
        "\n",
        "else:\n",
        "    with open(\"all_docs_updated.pkl\", 'rb') as f:\n",
        "        all_docs = pkl.load(f)      # load the updated all_docs from pkl file, containing the landmarks and NER results in the metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## ========== Chroma ==========\n",
        "# from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# sentence_transformer_embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "# print(\"Initialized SentenceTransformer embeddings.\")\n",
        "\n",
        "# # Load all documents into Chroma\n",
        "# db = Chroma.from_documents(all_docs, sentence_transformer_embeddings, persist_directory=\"./chroma_db_clean_huggingface\")\n",
        "# print('All documents loaded and embedded.(huggingface)')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
