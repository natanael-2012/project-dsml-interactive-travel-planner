{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def gpt_extract_info(user_input, current_step, conversation_state):\n",
    "    \"\"\"\n",
    "    Extract multiple pieces of information from the user's input.\n",
    "    For example, dates, interests, locations, and questions.\n",
    "    \"\"\"\n",
    "    if current_step == \"start\":\n",
    "        prompt = f\"\"\"\n",
    "        Analyze the user input and extract relevant information such as travel dates, interests and location information.\n",
    "\n",
    "        For example, if the user input is \"I want to go to Cabo Rojo on February 15 to go to the beach\"\n",
    "        The model should return the structured information in a JSON format:\n",
    "        {{  \"travel_dates\": [\"2025-02-15\"],\n",
    "            \"interests\": \"beaches, Cabo Rojo\" }}\n",
    "\n",
    "        The date should be in YYYY-MM-DD for it to be valid.\n",
    "            \n",
    "        If the user input is \"Hello\" just return\n",
    "        {{  \"travel_dates\": null,\n",
    "            \"interests\": null }}\n",
    "\n",
    "        If they input \"Hello. Where can i hike?\", return \n",
    "        {{  \"travel_dates\": null,\n",
    "            \"interests\": \"hiking\" }}\n",
    "        Today's date {str(datetime.date.today())}\n",
    "        User input: {user_input}\n",
    "        \"\"\"\n",
    "    elif current_step == \"received_interests\":\n",
    "        prompt = f\"\"\"Analyze the user input and extract their interests. \n",
    "        For example, if the user input is \"I like hiking and beaches\", the model should return the structured information in a JSON format: {{\"interests\": \"hiking, beaches\"}}. \n",
    "        If the user input is \"Hello. Where can I hike?\", the model should return {{\"interests\": \"hiking\"}}. \n",
    "        User input: {user_input}\"\"\"\n",
    "\n",
    "    elif current_step == \"received_location\":\n",
    "        prompt = f\"\"\"Analyze the user input and decide if the user mentioned a location they want to go to.\n",
    "        For example, if the user input is \"I want to go to Cabo Rojo\", the model should return the structured information in a JSON format:{{\"current_location\":\"Cabo Rojo\"}}.\n",
    "        If they answer \"yes\", they may be responding to the previous question. Add the place to the current location. Here is the chat history: {str(conversation_state['messages'])}\n",
    "        if they respond anything weird unrelated to the conversation, also add the last place suggested in the conversation. Remember, the format of the output is JSON, so the output should be like this: {{\"current_location\":\"location\"}}.\n",
    "        User input: {user_input}\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\n",
    "            'role': 'user', \n",
    "            'content':prompt\n",
    "            }],\n",
    "        response_format={\n",
    "            \"type\": \"json_object\"\n",
    "            },\n",
    "        max_tokens=200,\n",
    "        temperature=0.5\n",
    "    )\n",
    "    # print(gpt_response)\n",
    "    gpt_response=json.loads(response.choices[0].message.content)\n",
    "    \n",
    "    print(\"inside gpt_extract_info\")\n",
    "    \n",
    "    return gpt_response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_weather_dependent(location):\n",
    "    \"\"\"\n",
    "    Ask GPT whether a location is weather dependent or not.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Is the location '{location}' highly dependent on weather conditions? (e.g., outdoor activities, beach, hiking). \n",
    "    For example, if the location is El Morrow, the model should return True. If its a museum, the model should return False.\n",
    "    Return a JSON object with the key 'weather_dependent' and the boolean values of True (for highly dependant) or False (for not highly dependant).\n",
    "    \n",
    "    Once again: is the location '{location}' highly dependent on weather conditions?\"\"\"\n",
    "\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\n",
    "            'role': 'user', \n",
    "            'content':prompt\n",
    "            }],\n",
    "        response_format={\n",
    "            \"type\": \"json_object\"\n",
    "            },\n",
    "        max_tokens=200,\n",
    "        temperature=0.5\n",
    "    )\n",
    "    \n",
    "    gpt_response=json.loads(response.choices[0].message.content)\n",
    "    print(\"inside is_weather_dependent\")\n",
    "    return gpt_response['weather_dependent']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_weather(location, travel_dates):\n",
    "    \"\"\"\n",
    "    Ask GPT or an API to check the weather for the given location and travel dates.\n",
    "    \"\"\"\n",
    "    weather = get_weather(str(location), str(travel_dates[0]))\n",
    "    print(weather)\n",
    "    prompt = f\"\"\"You will tell me if the weather will be bad for outside activities. Respond in JSON format, with key \"bad_weather\" and boolean value.\n",
    "        The temperature will be of {weather['temp']['value']} deg Farenheit. The humidity level is of {weather[\"humidity\"]['value']}%. \n",
    "        A brief description of the weather is: {weather['weather']}.\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\n",
    "            'role': 'user', \n",
    "            'content':prompt\n",
    "            }],\n",
    "        response_format={\n",
    "            \"type\": \"json_object\"\n",
    "            },\n",
    "        max_tokens=200,\n",
    "        temperature=0.5\n",
    "    )\n",
    "    \n",
    "    gpt_response=json.loads(response.choices[0].message.content)\n",
    "    return gpt_response['bad_weather']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confirm_action(user_input, current_step):\n",
    "    \"\"\"\n",
    "    Use GPT to detect if the user confirms or rejects an action.\n",
    "    For example, locking a location or proceeding with a decision.\n",
    "    \"\"\"\n",
    "    print(\"inside confirm_action\")\n",
    "    if current_step == 'ask_lock_location' or current_step == 'lock_or_change':\n",
    "        prompt = f\"\"\" The user was asked if they want to lock the location. Based on their input: '{user_input}', does the user want to lock the location?\"\"\"\n",
    "    elif current_step == \"end_or_suggest_alternatives\":\n",
    "        prompt = f\"\"\" The user was asked if they want to add another visit. Based on their input: '{user_input}', does the user want to add another visit?\"\"\"\n",
    "\n",
    "    prompt += \"Return a JSON object with the key 'confirm' and the boolean values of True (for confirmation) or False (for rejection).\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\n",
    "            'role': 'user', \n",
    "            'content': prompt\n",
    "            }],\n",
    "        response_format={\n",
    "            \"type\": \"json_object\"\n",
    "            },\n",
    "        max_tokens=50,\n",
    "        temperature=0.5\n",
    "    )\n",
    "\n",
    "    gpt_response = json.loads(response.choices[0].message.content)\n",
    "    return gpt_response['confirm']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orchestrator(user_input, current_step, conversation_state):\n",
    "    \"\"\"\n",
    "    Orchestrates the conversation based on the current step and user input.\n",
    "\n",
    "    Parameters:\n",
    "    user_input (str): The input from the user.\n",
    "    current_step (str): The current step in the conversation.\n",
    "\n",
    "    Returns:\n",
    "    str: The next step in the conversation.\n",
    "    \"\"\"\n",
    "    # Define steps of conversation (flow)\n",
    "    if current_step == \"start\":\n",
    "        \n",
    "        # Get user input and check for details\n",
    "        detected_info = gpt_extract_info(user_input, current_step, conversation_state)  ### fix gpt response format\n",
    "\n",
    "        if detected_info[\"travel_dates\"] and detected_info[\"interests\"]:  # If both dates and interests are detected\n",
    "            conversation_state[\"travel_dates\"] = detected_info[\"travel_dates\"]\n",
    "            conversation_state[\"interests\"] = detected_info[\"interests\"]\n",
    "            return \"suggest_locations\", conversation_state\n",
    "        \n",
    "        elif detected_info[\"travel_dates\"]:  # If only dates are detected\n",
    "            conversation_state[\"travel_dates\"] = detected_info[\"travel_dates\"]\n",
    "            return \"ask_interests\", conversation_state\n",
    "        \n",
    "        elif detected_info[\"interests\"]:  # If only interests are detected\n",
    "            conversation_state[\"interests\"] = detected_info[\"interests\"]\n",
    "        # First step: ask for travel dates\n",
    "        return \"ask_travel_dates\", conversation_state\n",
    "    \n",
    "    elif current_step == \"received_dates\":\n",
    "\n",
    "        if conversation_state.get(\"interests\"):\n",
    "            # If interests are already detected, suggest locations\n",
    "            return \"suggest_locations\", conversation_state\n",
    "        \n",
    "        # Next step: ask for interests\n",
    "        return \"ask_interests\", conversation_state\n",
    "    \n",
    "    elif current_step == \"received_interests\":\n",
    "        # save interests\n",
    "        detected_info = gpt_extract_info(user_input, current_step, conversation_state)              ######### fix gpt response format\n",
    "        conversation_state[\"interests\"] = detected_info[\"interests\"]\n",
    "\n",
    "        # Now suggest locations based on interests\n",
    "        return \"suggest_locations\", conversation_state\n",
    "    ########### adding other steps that chatgpt didnt suggest#################\n",
    "    elif current_step == \"received_location\":\n",
    "        current_location = gpt_extract_info(user_input, current_step, conversation_state)   ##### fix gpt response format ???? What did i want to do here\n",
    "        conversation_state[\"current_location\"] = current_location\n",
    "        # Answer user questions about location\n",
    "        return \"answer_questions\", conversation_state\n",
    "    elif current_step == \"ask_lock_location\":\n",
    "        #after answering questions, ask if user wants to lock in location\n",
    "\n",
    "        ################################\n",
    "        want_to_lock = confirm_action(user_input, current_step) ######### fix gpt response format\n",
    "\n",
    "        #we asked if \"they want to go there.\" if they say yes, we lock in location (temporarily)\n",
    "        \n",
    "        if want_to_lock: #if they want to go there...\n",
    "            # check if location is weather dependant (maybe another gpt call and they respond {\"weather dependant\": True})\n",
    "            weather_dependant = is_weather_dependent(conversation_state[\"current_location\"])        # fix gpt response format\n",
    "            if weather_dependant:\n",
    "                #check weather for date\n",
    "                bad_weather = check_weather(conversation_state[\"current_location\"], conversation_state[\"travel_dates\"]) # fix gpt response format\n",
    "                if bad_weather:\n",
    "                    return \"bad_weather\", conversation_state\n",
    "            else:\n",
    "                return \"lock_location\", conversation_state\n",
    "        # if they say no, we suggest other locations\n",
    "        else:\n",
    "            return \"suggest_alternatives\" , conversation_state# create new step for this\"\n",
    "    elif current_step == \"lock_or_change\":\n",
    "        # there was bad weather and we asked the user if they wanted to lock the location.\n",
    "        want_to_lock = confirm_action(user_input, current_step)           #########fix gpt response format\n",
    "        if want_to_lock:\n",
    "            conversation_state[\"locked_locations\"].append(conversation_state[\"current_location\"])\n",
    "            return \"lock_location\", conversation_state\n",
    "        else:\n",
    "            return \"suggest_alternatives\", conversation_state\n",
    "\n",
    "    elif current_step == \"suggest_other_locations\":\n",
    "        # suggest other locations\n",
    "        return \"suggest_locations\", conversation_state\n",
    "    \n",
    "    elif current_step == \"end_or_suggest_alternatives\":\n",
    "        # we asked the user if the would like to go anywhere else\n",
    "        want_to_go = confirm_action(user_input, current_step)           #########fix gpt response format\n",
    "        if want_to_go:\n",
    "            return \"suggest_locations\", conversation_state\n",
    "        else:\n",
    "            return \"end_conversation\", conversation_state\n",
    "        \n",
    "    elif current_step == \"return_list_of_locked_locations\":\n",
    "        return \"give_list\", conversation_state\n",
    "    ##############################################\n",
    "    else:\n",
    "        # Fall back to default\n",
    "        return \"default_response\", conversation_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(user_input, instructions ,conversation_state, rag_response = None):\n",
    "    prompt= f\"\"\"\n",
    "        You are a bot that helps with tourism in Puerto Rico. The user said {user_input}.\n",
    "        This are some basic instructions for you, to answer to the user {instructions}.\n",
    "        Please be nice and professional, and keep the flow of the conversation.\n",
    "    \"\"\"\n",
    "    if rag_response:\n",
    "        prompt += f\"Use these RAGs we have for answering: {rag_response}\"\n",
    "        \n",
    "    message_history = conversation_state.get(\"messages\", [])\n",
    "    messages = message_history + [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "        max_tokens=500,\n",
    "        temperature=1\n",
    "    )\n",
    "    print(\"inside chat\")\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def communicator(orchestrator_action, user_input, conversation_state):\n",
    "    \"\"\"\n",
    "    Communicates with the user based on the orchestrator's action.\n",
    "\n",
    "    Parameters:\n",
    "    orchestrator_action (str): The action recommended by the orchestrator.\n",
    "    user_input (str): The input from the user.\n",
    "\n",
    "    Returns:\n",
    "    str: The response to the user.\n",
    "    \"\"\"\n",
    "    response = \"\"\n",
    "\n",
    "    if orchestrator_action == \"ask_travel_dates\":\n",
    "        # Use GPT-4o-mini to rephrase the question\n",
    "        response= chat(user_input, \"ask for the users travel dates\", conversation_state)\n",
    "    elif orchestrator_action == \"ask_interests\":\n",
    "        # GPT-4o-mini rephrasing\n",
    "        response= chat(user_input, \"Ask what kind of places do you they want to visit, like beaches, museums or other you want to say\", conversation_state)\n",
    "        \n",
    "    elif orchestrator_action == \"suggest_locations\":\n",
    "        # Suggest locations based on interests (USE RAG)\n",
    "        if not conversation_state.get(\"suggested_locations\"):\n",
    "            rag_response = db.similarity_search(user_input, k=7, filter={'source':'landmarks'})\n",
    "            conversation_state[\"suggested_locations\"] = rag_response\n",
    "            print(rag_response[0].metadata)\n",
    "        else:\n",
    "            index = conversation_state[\"suggested_locations\"].index(conversation_state[\"current_location\"])\n",
    "            conversation_state[\"suggested_locations\"].pop(index)\n",
    "            rag_response.pop(index)\n",
    "        \n",
    "        # GPT-4o-mini rephrasing to suggest locations based on RAG\n",
    "        response = chat(user_input, \"Suggest locations based on the user's preferences (use the RAGs). Give nothing but a brief description of one of them (the first one from the rag), but dont give a description for the others. Ask if they want to visit the one you gave the description\", conversation_state, rag_response)\n",
    "    elif orchestrator_action == \"answer_questions\":\n",
    "        # Answer questions about the location\n",
    "        info = db.similarity_search(user_input) \n",
    "        response = chat(user_input, f\"Answer the user's questions about (or simply give info) {conversation_state['current_location']}, and ask if they want to visit.\", conversation_state)\n",
    "    elif orchestrator_action == \"bad_weather\":\n",
    "        # Inform user about bad weather\n",
    "        response = chat(user_input, \"Inform the user that the weather is bad for their travel dates and ask if they still want to proceed with the location.\", conversation_state)\n",
    "    elif orchestrator_action == \"lock_location\":\n",
    "        # Lock the location\n",
    "        response = chat(user_input, \"Confirm that you've locked the selected location for their trip and ask if they want to choose more locations.\", conversation_state)\n",
    "    elif orchestrator_action == \"suggest_alternatives\":\n",
    "        # Suggest alternative locations\n",
    "        response = chat(user_input, \"Suggest alternative locations the user might be interested in if the previous location wasn't a good fit.\", conversation_state)\n",
    "    elif orchestrator_action == \"end_conversation\":\n",
    "        # End the conversation\n",
    "        response = chat(user_input, \"Thank the user and wish them a great trip. Prepare to end the conversation.\", conversation_state)\n",
    "    elif orchestrator_action == \"give_list\":\n",
    "        # Provide the list of locked locations\n",
    "        response = chat(user_input, \"Give the user a list of the locations they have locked in for their trip.\", conversation_state)\n",
    "    else:\n",
    "        # Default fallback\n",
    "        response = chat(user_input, \"I'm not sure how to respond to this action. Ask for clarification from the user.\", conversation_state)\n",
    "    \n",
    "    conversation_state[\"messages\"].append({'role':\"user\", \"content\": user_input})\n",
    "    conversation_state[\"messages\"].append({'role':\"system\", \"content\": response})\n",
    "\n",
    "    print(f\"orchestrator_action: {orchestrator_action}\")\n",
    "\n",
    "    return response, conversation_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cur_step (orchestrator_action):\n",
    "     # Update the flow based on the orchestrator's action\n",
    "    if orchestrator_action == \"ask_travel_dates\":\n",
    "        return \"received_dates\"\n",
    "    elif orchestrator_action == \"ask_interests\":\n",
    "        return \"received_interests\"\n",
    "    elif orchestrator_action == \"suggest_locations\":\n",
    "        #########################\n",
    "        return \"received_location\"\n",
    "    elif orchestrator_action == \"answer_questions\":\n",
    "        return \"ask_lock_location\"\n",
    "    elif orchestrator_action == \"bad_weather\":\n",
    "        return \"lock_or_change\"\n",
    "    elif orchestrator_action == \"lock_location\":\n",
    "        return \"end_or_suggest_alternatives\"\n",
    "\n",
    "    elif orchestrator_action == \"suggest_alternatives\":\n",
    "        return \"suggest_other_locations\"\n",
    "    elif orchestrator_action == \"end_conversation\":\n",
    "        return \"return_list_of_locked_locations\"\n",
    "    elif orchestrator_action == \"give_list\":  \n",
    "        return \"end\"\n",
    "    return \"default_response\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Natanael\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Huggingface database loaded.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from weatherapi import get_weather\n",
    "import datetime\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "\n",
    "# Load the environment variables\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "WEATHER_API_KEY = os.getenv(\"WEATHER_API_KEY\")\n",
    "\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    api_key=OPENAI_API_KEY\n",
    ")\n",
    "\n",
    "sentence_transformer_embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "# print(\"Initialized SentenceTransformer embeddings.\")\n",
    "\n",
    "# print(\"\\nLoading database...\")\n",
    "db = Chroma(persist_directory='./chroma_db', embedding_function=sentence_transformer_embeddings)\n",
    "print(\"Huggingface database loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "q =db.similarity_search(\"I want to go to Paris next week.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'date': 'February 14, 1920',\n",
       " 'filename': '19200214_1.txt',\n",
       " 'locations': 'Soissons, Santo Domingo, Puerto Rico, Mayagüez, puerto rico, San Juan, Caguas, Havana, San Quintín',\n",
       " 'source': 'news'}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \n",
      "inside gpt_extract_info\n",
      "inside chat\n",
      "Bot: Of course! I'd be happy to help you with your travel plans to Puerto Rico. Could you please share your travel dates? This will allow me to provide you with the best recommendations for your visit.\n",
      "> Tomorrow\n",
      "inside chat\n",
      "Bot: That sounds exciting! Since you'll be traveling tomorrow, I’d love to help you make the most of your trip to Puerto Rico. What kind of places are you interested in visiting? Do you prefer beautiful beaches, cultural museums, historical sites, or perhaps outdoor adventures? Let me know, and I can tailor some recommendations for you!\n",
      "> i want to go to a museum\n",
      "inside gpt_extract_info\n",
      "{'filename': 'museo_de_la_historia_de_ponce.txt', 'landmark': 'Museo de la Historia de Ponce', 'latitude': 18.012546, 'longitude': -66.611729, 'municipality': 'Ponce', 'source': 'landmarks', 'url': 'https://en.wikipedia.org/wiki/Museo_de_la_Historia_de_Ponce'}\n",
      "inside chat\n",
      "Bot: One great option for you is the **Museo de la Historia de Ponce**, located in the historic Casa Salazar-Candal. This museum showcases the history of Ponce, including its ecology, economy, architecture, and elements of daily life. It aims to promote the research and preservation of the city's heritage.\n",
      "\n",
      "Would you be interested in visiting the Museo de la Historia de Ponce?\n",
      "> no\n",
      "inside gpt_extract_info\n",
      "inside chat\n",
      "Bot: Got it! In that case, if you're open to exploring other museums, how about the **Museo de Arte de Ponce**? It is one of the most significant art museums in the Caribbean, featuring an impressive collection of European and Puerto Rican art. The museum is housed in a beautiful building and often hosts engaging temporary exhibitions as well.\n",
      "\n",
      "Would you like to visit the Museo de Arte de Ponce, or do you have another type of museum in mind?\n",
      "> yes\n",
      "inside confirm_action\n",
      "inside is_weather_dependent\n",
      "inside chat\n",
      "Bot: Great choice! I’ve locked in the **Museo de Arte de Ponce** for your visit. It's a wonderful place that will give you a deeper appreciation of art in Puerto Rico.\n",
      "\n",
      "Would you like to choose any more locations or activities to include in your itinerary while you're in Puerto Rico? Let me know how else I can assist you!\n",
      "> yes\n",
      "inside confirm_action\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "{'current_location': 'Museo de la Historia de Ponce'} is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[122], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_input\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m orchestrator_action, conversation_state \u001b[38;5;241m=\u001b[39m orchestrator(user_input, current_step, conversation_state)\n\u001b[1;32m---> 19\u001b[0m response, conversation_state \u001b[38;5;241m=\u001b[39m communicator(orchestrator_action, user_input, conversation_state)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBot: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Update the flow based on the orchestrator's action\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[121], line 28\u001b[0m, in \u001b[0;36mcommunicator\u001b[1;34m(orchestrator_action, user_input, conversation_state)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28mprint\u001b[39m(rag_response[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmetadata)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 28\u001b[0m     index \u001b[38;5;241m=\u001b[39m conversation_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuggested_locations\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mindex(conversation_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurrent_location\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     29\u001b[0m     conversation_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuggested_locations\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mpop(index)\n\u001b[0;32m     30\u001b[0m     rag_response\u001b[38;5;241m.\u001b[39mpop(index)\n",
      "\u001b[1;31mValueError\u001b[0m: {'current_location': 'Museo de la Historia de Ponce'} is not in list"
     ]
    }
   ],
   "source": [
    "# Example user input and flow\n",
    "conversation_state = {\n",
    "    \"travel_dates\": None,\n",
    "    \"interests\": None,\n",
    "    \"locked_locations\": [],   # i need to save the coordinates of the locations\n",
    "    \"suggested_locations\": [],\n",
    "    \"current_location\": None,\n",
    "    \"messages\": []  # To store the full conversation history\n",
    "}\n",
    "rag_response = None\n",
    "\n",
    "current_step = \"start\"\n",
    "while current_step != \"end\":\n",
    "    user_input = input(\"You: \")  # Get user input\n",
    "    if(user_input == \"exit\"):\n",
    "        break\n",
    "    print(f\"> {user_input}\")\n",
    "    orchestrator_action, conversation_state = orchestrator(user_input, current_step, conversation_state)\n",
    "    response, conversation_state = communicator(orchestrator_action, user_input, conversation_state)\n",
    "    print(f\"Bot: {response}\")\n",
    "    \n",
    "    # Update the flow based on the orchestrator's action\n",
    "    if orchestrator_action == \"ask_travel_dates\":\n",
    "        current_step = \"received_dates\"\n",
    "    elif orchestrator_action == \"ask_interests\":\n",
    "        current_step = \"received_interests\"\n",
    "    elif orchestrator_action == \"suggest_locations\":\n",
    "        #########################\n",
    "        current_step = \"received_location\"\n",
    "    elif orchestrator_action == \"answer_questions\":\n",
    "        current_step = \"ask_lock_location\"\n",
    "    elif orchestrator_action == \"bad_weather\":\n",
    "        current_step = \"lock_or_change\"\n",
    "    elif orchestrator_action == \"lock_location\":\n",
    "        current_step = \"end_or_suggest_alternatives\"\n",
    "\n",
    "    elif orchestrator_action == \"suggest_alternatives\":\n",
    "        current_step = \"suggest_other_locations\"\n",
    "    elif orchestrator_action == \"end_conversation\":\n",
    "        current_step = \"return_list_of_locked_locations\"\n",
    "    elif orchestrator_action == \"give_list\":  \n",
    "        current_step = \"end\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
