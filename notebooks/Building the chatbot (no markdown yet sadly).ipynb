{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def gpt_extract_info(user_input, current_step, conversation_state):\n",
    "    \"\"\"\n",
    "    Extract multiple pieces of information from the user's input.\n",
    "    For example, dates, interests, locations, and questions.\n",
    "    \"\"\"\n",
    "    if current_step == \"start\":\n",
    "        prompt = f\"\"\"\n",
    "        Analyze the user input and extract relevant information such as travel dates, interests and location information.\n",
    "\n",
    "        For example, if the user input is \"I want to go to Cabo Rojo on February 15 to go to the beach\"\n",
    "        The model should return the structured information in a JSON format:\n",
    "        {{  \"travel_dates\": [\"2025-02-15\"],\n",
    "            \"interests\": \"beaches, Cabo Rojo\" }}\n",
    "\n",
    "        The date should be in YYYY-MM-DD for it to be valid.\n",
    "            \n",
    "        If the user input is \"Hello\" just return\n",
    "        {{  \"travel_dates\": null,\n",
    "            \"interests\": null }}\n",
    "\n",
    "        If they input \"Hello. Where can i hike?\", return \n",
    "        {{  \"travel_dates\": null,\n",
    "            \"interests\": \"hiking\" }}\n",
    "        Today's date {str(datetime.date.today())}\n",
    "        User input: {user_input}\n",
    "        \"\"\"\n",
    "    elif current_step == \"received_interests\":\n",
    "        prompt = f\"\"\"Analyze the user input and extract their interests. \n",
    "        For example, if the user input is \"I like hiking and beaches\", the model should return the structured information in a JSON format: {{\"interests\": \"hiking, beaches\"}}. \n",
    "        If the user input is \"Hello. Where can I hike?\", the model should return {{\"interests\": \"hiking\"}}. \n",
    "        User input: {user_input}\"\"\"\n",
    "\n",
    "    elif current_step == \"received_location\":\n",
    "        prompt = f\"\"\"Analyze the user input and decide if the user mentioned a location they want to go to.\n",
    "        For example, if the user input is \"I want to go to Cabo Rojo\", the model should return the structured information in a JSON format: {{\"current_location\": \"Cabo Rojo\"}}.\n",
    "        If they answer \"yes\", they may be responding to the previous question. Here is the context of {conversation_state['messages'][-2:]} Add the place to the current location.\n",
    "        User input: {user_input}\"\"\"\n",
    "\n",
    "    elif current_step == \"ask_accept_location\":\n",
    "        prompt = f\"\"\"Analyze the user input and decide if the user accepted or declined the suggested location.\n",
    "        If they accept, return a JSON like this {{'user_decision': 'accept'}}. If they decline, return a JSON like this {{'user_decision': 'decline'}}.\n",
    "        User input: {user_input}\"\"\"\n",
    "\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\n",
    "            'role': 'user', \n",
    "            'content':prompt\n",
    "            }],\n",
    "        response_format={\n",
    "            \"type\": \"json_object\"\n",
    "            },\n",
    "        max_tokens=200,\n",
    "        temperature=0.5\n",
    "    )\n",
    "    # print(gpt_response)\n",
    "    gpt_response=json.loads(response.choices[0].message.content)\n",
    "    \n",
    "    print(\"inside gpt_extract_info\")\n",
    "    \n",
    "    return gpt_response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_weather_dependent(location):\n",
    "    \"\"\"\n",
    "    Ask GPT whether a location is weather dependent or not.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Is the location '{location}' highly dependent on weather conditions? (e.g., outdoor activities, beach, hiking). \n",
    "    For example, if the location is El Morrow, the model should return True. If its a museum, the model should return False.\n",
    "    Return a JSON object with the key 'weather_dependent' and the boolean values of True (for highly dependant) or False (for not highly dependant).\n",
    "    \n",
    "    Once again: is the location '{location}' highly dependent on weather conditions?\"\"\"\n",
    "\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\n",
    "            'role': 'user', \n",
    "            'content':prompt\n",
    "            }],\n",
    "        response_format={\n",
    "            \"type\": \"json_object\"\n",
    "            },\n",
    "        max_tokens=200,\n",
    "        temperature=0.5\n",
    "    )\n",
    "    \n",
    "    gpt_response=json.loads(response.choices[0].message.content)\n",
    "    print(\"inside is_weather_dependent\")\n",
    "    return gpt_response['weather_dependent']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_weather(location, travel_dates):\n",
    "    \"\"\"\n",
    "    Ask GPT or an API to check the weather for the given location and travel dates.\n",
    "    \"\"\"\n",
    "    weather = get_weather(str(location), str(travel_dates[0]))\n",
    "    print(weather)\n",
    "    prompt = f\"\"\"You will tell me if the weather will be bad for outside activities. Respond in JSON format, with key \"bad_weather\" and boolean value.\n",
    "        The temperature will be of {weather['temp']['value']} deg Farenheit. The humidity level is of {weather[\"humidity\"]['value']}%. \n",
    "        A brief description of the weather is: {weather['weather']}.\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\n",
    "            'role': 'user', \n",
    "            'content':prompt\n",
    "            }],\n",
    "        response_format={\n",
    "            \"type\": \"json_object\"\n",
    "            },\n",
    "        max_tokens=200,\n",
    "        temperature=0.5\n",
    "    )\n",
    "    \n",
    "    gpt_response=json.loads(response.choices[0].message.content)\n",
    "    return gpt_response['bad_weather']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confirm_action(user_input, current_step):\n",
    "    \"\"\"\n",
    "    Use GPT to detect if the user confirms or rejects an action.\n",
    "    For example, locking a location or proceeding with a decision.\n",
    "    \"\"\"\n",
    "    print(\"inside confirm_action\")\n",
    "    if current_step == 'ask_lock_location' or current_step == 'lock_or_change':\n",
    "        prompt = f\"\"\" The user was asked if they want to lock the location. Based on their input: '{user_input}', does the user want to lock the location?\"\"\"\n",
    "    elif current_step == \"end_or_suggest_alternatives\":\n",
    "        prompt = f\"\"\" The user was asked if they want to add another visit. Based on their input: '{user_input}', does the user want to add another visit?\"\"\"\n",
    "\n",
    "    prompt += \"Return a JSON object with the key 'confirm' and the boolean values of True (for confirmation) or False (for rejection).\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\n",
    "            'role': 'user', \n",
    "            'content': prompt\n",
    "            }],\n",
    "        response_format={\n",
    "            \"type\": \"json_object\"\n",
    "            },\n",
    "        max_tokens=50,\n",
    "        temperature=0.5\n",
    "    )\n",
    "\n",
    "    gpt_response = json.loads(response.choices[0].message.content)\n",
    "    return gpt_response['confirm']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orchestrator(user_input, current_step, conversation_state):\n",
    "    \"\"\"\n",
    "    Orchestrates the conversation based on the current step and user input.\n",
    "\n",
    "    Parameters:\n",
    "    user_input (str): The input from the user.\n",
    "    current_step (str): The current step in the conversation.\n",
    "\n",
    "    Returns:\n",
    "    str: The next step in the conversation.\n",
    "    \"\"\"\n",
    "    # Define steps of conversation (flow)\n",
    "    if current_step == \"start\":\n",
    "        \n",
    "        # Get user input and check for details\n",
    "        detected_info = gpt_extract_info(user_input, current_step, conversation_state)  ### fix gpt response format\n",
    "\n",
    "        if detected_info[\"travel_dates\"] and detected_info[\"interests\"]:  # If both dates and interests are detected\n",
    "            conversation_state[\"travel_dates\"] = detected_info[\"travel_dates\"]\n",
    "            conversation_state[\"interests\"] = detected_info[\"interests\"]\n",
    "            return \"suggest_locations\", conversation_state\n",
    "        \n",
    "        elif detected_info[\"travel_dates\"]:  # If only dates are detected\n",
    "            conversation_state[\"travel_dates\"] = detected_info[\"travel_dates\"]\n",
    "            return \"ask_interests\", conversation_state\n",
    "        \n",
    "        elif detected_info[\"interests\"]:  # If only interests are detected\n",
    "            conversation_state[\"interests\"] = detected_info[\"interests\"]\n",
    "        # First step: ask for travel dates\n",
    "        return \"ask_travel_dates\", conversation_state\n",
    "    \n",
    "    elif current_step == \"received_dates\":\n",
    "\n",
    "        if conversation_state.get(\"interests\"):\n",
    "            # If interests are already detected, suggest locations\n",
    "            return \"suggest_locations\", conversation_state\n",
    "        \n",
    "        # Next step: ask for interests\n",
    "        return \"ask_interests\", conversation_state\n",
    "    \n",
    "    elif current_step == \"received_interests\":\n",
    "        # save interests\n",
    "        detected_info = gpt_extract_info(user_input, current_step, conversation_state)              ######### fix gpt response format\n",
    "        conversation_state[\"interests\"] = detected_info[\"interests\"]\n",
    "\n",
    "        # Now suggest locations based on interests\n",
    "        return \"suggest_locations\", conversation_state\n",
    "    ########### adding other steps that chatgpt didnt suggest#################\n",
    "    elif current_step == \"received_location\":\n",
    "    # Extract current location or confirm the last suggested one\n",
    "                ################## here\n",
    "        curr_loc = gpt_extract_info(user_input, current_step, conversation_state)\n",
    "        curr_loc=(str(curr_loc['current_location'])+\" \"+conversation_state['interests'])\n",
    "        print(curr_loc)\n",
    "        current_location = db.similarity_search(curr_loc, k=1, filter={'source':'landmark'})\n",
    "        # current_location = gpt_extract_info(user_input, current_step, conversation_state)\n",
    "        \n",
    "        if current_location:\n",
    "            conversation_state[\"current_location\"] = current_location\n",
    "        \n",
    "        # Ask if the user accepts the suggested location\n",
    "        return \"ask_accept_location\", conversation_state\n",
    "\n",
    "    elif current_step == \"ask_accept_location\":\n",
    "        # Extract user's decision (accept or decline)\n",
    "        user_decision = gpt_extract_info(user_input, current_step, conversation_state)\n",
    "        \n",
    "        if user_decision == \"accept\":\n",
    "            # Move forward to lock the location or ask for further details\n",
    "            return \"lock_location\", conversation_state\n",
    "        else:\n",
    "            # If the user declines, go back and suggest another location\n",
    "            return \"suggest_locations\", conversation_state\n",
    "\n",
    "    elif current_step == \"ask_lock_location\":\n",
    "        #after answering questions, ask if user wants to lock in location\n",
    "\n",
    "        ################################\n",
    "        want_to_lock = confirm_action(user_input, current_step) ######### fix gpt response format\n",
    "\n",
    "        #we asked if \"they want to go there.\" if they say yes, we lock in location (temporarily)\n",
    "        \n",
    "        if want_to_lock: #if they want to go there...\n",
    "            # check if location is weather dependant (maybe another gpt call and they respond {\"weather dependant\": True})\n",
    "            weather_dependant = is_weather_dependent(conversation_state[\"current_location\"])        # fix gpt response format\n",
    "            if weather_dependant:\n",
    "                #check weather for date\n",
    "                bad_weather = check_weather(conversation_state[\"current_location\"], conversation_state[\"travel_dates\"]) # fix gpt response format\n",
    "                if bad_weather:\n",
    "                    return \"bad_weather\", conversation_state\n",
    "            else:\n",
    "                return \"lock_location\", conversation_state\n",
    "        # if they say no, we suggest other locations\n",
    "        else:\n",
    "            #suggest locations\n",
    "            return \"suggest_locations\", conversation_state\n",
    "            # return \"suggest_alternatives\" , conversation_state# create new step for this\"\n",
    "    elif current_step == \"lock_or_change\":\n",
    "        # there was bad weather and we asked the user if they wanted to lock the location.\n",
    "        want_to_lock = confirm_action(user_input, current_step)           #########fix gpt response format\n",
    "        if want_to_lock:\n",
    "            conversation_state[\"locked_locations\"].append(conversation_state[\"current_location\"])\n",
    "            return \"lock_location\", conversation_state\n",
    "        else:\n",
    "            return \"suggest_alternatives\", conversation_state\n",
    "\n",
    "    elif current_step == \"suggest_other_locations\":\n",
    "        # suggest other locations\n",
    "        return \"suggest_locations\", conversation_state\n",
    "    \n",
    "    elif current_step == \"end_or_suggest_alternatives\":\n",
    "        # we asked the user if the would like to go anywhere else\n",
    "        want_to_go = confirm_action(user_input, current_step)           #########fix gpt response format\n",
    "        if want_to_go:\n",
    "            return \"suggest_locations\", conversation_state\n",
    "        else:\n",
    "            return \"end_conversation\", conversation_state\n",
    "        \n",
    "    elif current_step == \"return_list_of_locked_locations\":\n",
    "        return \"give_list\", conversation_state\n",
    "    ##############################################\n",
    "    else:\n",
    "        # Fall back to default\n",
    "        return \"default_response\", conversation_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(user_input, instructions ,conversation_state, rag_response = None):\n",
    "    prompt= f\"\"\"\n",
    "        You are a bot that helps with tourism in Puerto Rico. The user said {user_input}.\n",
    "        This are some basic instructions for you, to answer to the user {instructions}.\n",
    "        Please be nice and professional, and keep the flow of the conversation.\n",
    "    \"\"\"\n",
    "    if rag_response:\n",
    "        prompt += f\"Use these RAGs we have for answering: {rag_response}\"\n",
    "        \n",
    "    message_history = conversation_state.get(\"messages\", [])\n",
    "    messages = message_history + [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "        max_tokens=500,\n",
    "        temperature=1\n",
    "    )\n",
    "    print(\"inside chat\")\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def communicator(orchestrator_action, user_input, conversation_state):\n",
    "    \"\"\"\n",
    "    Communicates with the user based on the orchestrator's action.\n",
    "\n",
    "    Parameters:\n",
    "    orchestrator_action (str): The action recommended by the orchestrator.\n",
    "    user_input (str): The input from the user.\n",
    "\n",
    "    Returns:\n",
    "    str: The response to the user.\n",
    "    \"\"\"\n",
    "    response = \"\"\n",
    "\n",
    "    if orchestrator_action == \"ask_travel_dates\":\n",
    "        # Use GPT-4o-mini to rephrase the question\n",
    "        response= chat(user_input, \"ask for the users travel dates\", conversation_state)\n",
    "    elif orchestrator_action == \"ask_interests\":\n",
    "        # GPT-4o-mini rephrasing\n",
    "        response= chat(user_input, \"Ask what kind of places do you they want to visit, like beaches, museums or other you want to say\", conversation_state)\n",
    "\n",
    "    elif orchestrator_action == \"ask_accept_location\":\n",
    "    # Rephrase the question to the user asking if they want to visit the current location\n",
    "        current_location = conversation_state.get(\"current_location\", \"the suggested location\")\n",
    "        response = chat(user_input, f\"Do you want to visit {current_location}? Please answer 'yes' or 'no'.\", conversation_state)\n",
    "        \n",
    "\n",
    "    elif orchestrator_action == \"suggest_locations\":\n",
    "        # Suggest locations based on interests (USE RAG)\n",
    "        if not conversation_state.get(\"suggested_locations\"):\n",
    "            rag_response = db.similarity_search(user_input, k=7, filter={'source': 'landmarks'})\n",
    "            conversation_state[\"suggested_locations\"] = rag_response\n",
    "        else:\n",
    "            # Remove the current location from the list if declined\n",
    "            current_location = conversation_state.get(\"current_location\")\n",
    "            conversation_state[\"suggested_locations\"] = [\n",
    "                loc for loc in conversation_state[\"suggested_locations\"] if loc != current_location\n",
    "            ]\n",
    "        \n",
    "        # GPT-4o-mini rephrasing to suggest locations based on RAG\n",
    "        response = chat(user_input, \"Here is another location you might like: (description of first from RAG). Would you like to visit this one?\", conversation_state)\n",
    "            \n",
    "   \n",
    "\n",
    "    elif orchestrator_action == \"answer_questions\":\n",
    "        # Answer questions about the location\n",
    "        info = db.similarity_search(user_input) \n",
    "        response = chat(user_input, f\"Answer the user's questions about (or simply give info) {conversation_state['current_location']}, and ask if they want to visit.\", conversation_state, info)\n",
    "    elif orchestrator_action == \"bad_weather\":\n",
    "        # Inform user about bad weather\n",
    "        response = chat(user_input, \"Inform the user that the weather is bad for their travel dates and ask if they still want to proceed with the location.\", conversation_state)\n",
    "    elif orchestrator_action == \"lock_location\":\n",
    "        # Lock the location\n",
    "                ####### here\n",
    "        conversation_state[\"locked_locations\"].append(conversation_state[\"current_location\"]) \n",
    "        response = chat(user_input, \"Confirm that you've locked the selected location for their trip and ask if they want to choose more locations.\", conversation_state)\n",
    "    elif orchestrator_action == \"suggest_alternatives\":\n",
    "        # Suggest alternative locations\n",
    "        response = chat(user_input, \"Suggest alternative locations the user might be interested in if the previous location wasn't a good fit.\", conversation_state)\n",
    "    elif orchestrator_action == \"end_conversation\":\n",
    "        # End the conversation\n",
    "        response = chat(user_input, \"Thank the user and wish them a great trip. Prepare to end the conversation.\", conversation_state)\n",
    "    elif orchestrator_action == \"give_list\":\n",
    "        # Provide the list of locked locations\n",
    "        lst = str(conversation_state.get(\"locked_locations\", []))\n",
    "        response = chat(user_input, f\"Give the user a list of the locations they have locked in for their trip. The locked locations are {lst}\", conversation_state)\n",
    "    else:\n",
    "        # Default fallback\n",
    "        response = chat(user_input, \"I'm not sure how to respond to this action. Ask for clarification from the user.\", conversation_state)\n",
    "    \n",
    "    conversation_state[\"messages\"].append({'role':\"user\", \"content\": user_input})\n",
    "    conversation_state[\"messages\"].append({'role':\"system\", \"content\": response})\n",
    "\n",
    "    print(f\"orchestrator_action: {orchestrator_action}\")\n",
    "\n",
    "    return response, conversation_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cur_step (orchestrator_action):\n",
    "     # Update the flow based on the orchestrator's action\n",
    "    if orchestrator_action == \"ask_travel_dates\":\n",
    "        return \"received_dates\"\n",
    "    elif orchestrator_action == \"ask_interests\":\n",
    "        return \"received_interests\"\n",
    "    elif orchestrator_action == \"suggest_locations\":\n",
    "        #########################\n",
    "        return \"received_location\"\n",
    "    elif orchestrator_action == \"answer_questions\":\n",
    "        return \"ask_lock_location\"\n",
    "    elif orchestrator_action == \"bad_weather\":\n",
    "        return \"lock_or_change\"\n",
    "    elif orchestrator_action == \"lock_location\":\n",
    "        return \"end_or_suggest_alternatives\"\n",
    "\n",
    "    elif orchestrator_action == \"suggest_alternatives\":\n",
    "        return \"suggest_other_locations\"\n",
    "    elif orchestrator_action == \"end_conversation\":\n",
    "        return \"return_list_of_locked_locations\"\n",
    "    elif orchestrator_action == \"give_list\":  \n",
    "        return \"end\"\n",
    "    return \"default_response\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Natanael\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Huggingface database loaded.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from weatherapi import get_weather\n",
    "import datetime\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "\n",
    "# Load the environment variables\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "WEATHER_API_KEY = os.getenv(\"WEATHER_API_KEY\")\n",
    "\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    api_key=OPENAI_API_KEY\n",
    ")\n",
    "\n",
    "sentence_transformer_embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "# print(\"Initialized SentenceTransformer embeddings.\")\n",
    "\n",
    "# print(\"\\nLoading database...\")\n",
    "db = Chroma(persist_directory='./chroma_db', embedding_function=sentence_transformer_embeddings)\n",
    "print(\"Huggingface database loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> hello\n",
      "Current step: start\n",
      "inside gpt_extract_info\n",
      "inside chat\n",
      "orchestrator_action: ask_travel_dates\n",
      "Bot: Hello! Welcome to Puerto Rico tourism assistance. How can I help you today? If you're planning a trip, I'd love to know your travel dates so I can provide you with the best recommendations!\n"
     ]
    }
   ],
   "source": [
    "# Example user input and flow\n",
    "conversation_state = {\n",
    "    \"travel_dates\": None,\n",
    "    \"interests\": None,\n",
    "    \"locked_locations\": [],   # i need to save the coordinates of the locations\n",
    "    \"suggested_locations\": [],\n",
    "    \"current_location\": None,\n",
    "    \"messages\": []  # To store the full conversation history\n",
    "}\n",
    "rag_response = None\n",
    "\n",
    "current_step = \"start\"\n",
    "while current_step != \"end\":\n",
    "    user_input = input(\"You (tye 'exit' to close): \")  # Get user input\n",
    "    if user_input == \"\":\n",
    "        continue\n",
    "    if(user_input == \"exit\"):\n",
    "        break\n",
    "    \n",
    "    print(f\"> {user_input}\")\n",
    "    print(\"Current step:\", current_step)\n",
    "\n",
    "    orchestrator_action, conversation_state = orchestrator(user_input, current_step, conversation_state)\n",
    "    response, conversation_state = communicator(orchestrator_action, user_input, conversation_state)\n",
    "    print(f\"Bot: {response}\")\n",
    "    \n",
    "    # Update the flow based on the orchestrator's action\n",
    "    if orchestrator_action == \"ask_travel_dates\":\n",
    "        current_step = \"received_dates\"\n",
    "    elif orchestrator_action == \"ask_interests\":\n",
    "        current_step = \"received_interests\"\n",
    "    elif orchestrator_action == \"suggest_locations\":\n",
    "        #########################\n",
    "        current_step = \"received_location\"\n",
    "    elif orchestrator_action == \"ask_accept_location\":\n",
    "        current_step = \"ask_lock_location\"\n",
    "        # current_step = \"ask_accept_location\"        #ask lock location\n",
    "\n",
    "    elif orchestrator_action == \"bad_weather\":\n",
    "        current_step = \"lock_or_change\"\n",
    "    elif orchestrator_action == \"lock_location\":\n",
    "        current_step = \"end_or_suggest_alternatives\"\n",
    "\n",
    "    elif orchestrator_action == \"suggest_alternatives\":\n",
    "        current_step = \"suggest_other_locations\"\n",
    "    elif orchestrator_action == \"end_conversation\":\n",
    "        current_step = \"return_list_of_locked_locations\"\n",
    "    elif orchestrator_action == \"give_list\":  \n",
    "        current_step = \"end\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
