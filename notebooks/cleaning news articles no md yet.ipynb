{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The paths on this notebook are based on the root of the repository, but this notebook was moved afterwards. Make sure to fix the paths if you want to run the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3sx2JaTzBPzh"
      },
      "outputs": [],
      "source": [
        "# importing the required libraries. Some libraries run on colab while others run locally\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "from tqdm import tqdm\n",
        "from openai import OpenAI\n",
        "import pickle as pkl\n",
        "\n",
        "try:\n",
        "    from dotenv import load_dotenv\n",
        "    load_dotenv()\n",
        "    HF_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
        "    OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "    \n",
        "except:\n",
        "    from google.colab import drive\n",
        "    from google.colab import userdata\n",
        "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Set to True to run the pipelines. Since we stored the results in a file, \n",
        "# we can set this to False to avoid running the pipelines again\n",
        "RUN_PIPELINES = False   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LJcTvPEABPzk"
      },
      "outputs": [],
      "source": [
        "# from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WBsR5kABPzm",
        "outputId": "353d93b0-67ec-4064-c166-064e5799f941"
      },
      "outputs": [],
      "source": [
        "# same as above. If on colab, we need to mount the drive. If locally, we can set the path to the local directory\n",
        "\n",
        "# ========== Mount Google Drive ==========\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# ========== Path Configuration ==========\n",
        "# Update these paths according to your Google Drive structure\n",
        "    DRIVE_BASE = '/content/drive/MyDrive/'\n",
        "    EXTRACT_DIR = '/tmp/extracted'  # Using tmp for faster I/O\n",
        "\n",
        "except:\n",
        "    DRIVE_BASE = './outputs/'\n",
        "    EXTRACT_DIR = './tmp/extracted'  # Using tmp for faster I/O\n",
        "\n",
        "ZIP_PATH = \"./data/elmundo_chunked_es_page1_40years.zip\"\n",
        "OUTPUT_DIR = os.path.join(DRIVE_BASE, 'cleaned_articles1')\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(EXTRACT_DIR, exist_ok=True)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xl-0upqUBPzq"
      },
      "outputs": [],
      "source": [
        "# ========== File Extraction ==========\n",
        "def extract_files(zip_path, extract_dir):\n",
        "    \"\"\"\n",
        "    Extracts files from a zip archive to a directory.\n",
        "    \n",
        "    Args:\n",
        "    zip_path (str): Path to the zip archive.\n",
        "    extract_dir (str): Directory to extract the files to.\n",
        "    \"\"\"\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        # Extract nested structure\n",
        "        for file in zip_ref.namelist():\n",
        "            if file.endswith('.txt'):\n",
        "                zip_ref.extract(file, extract_dir)\n",
        "    print(\"*\" * 50)\n",
        "    print(f\"Extracted files to: {extract_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "76WAjMOFBPzs"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(\n",
        "    api_key=OPENAI_API_KEY\n",
        ")\n",
        "\n",
        "def correct_with_openai(text, filename, just_text = True, max_completion_tokens = 2048, temperature = 1, top_p = 1, frequency_penalty=0, presence_penalty=0,**kwargs):\n",
        "  \"\"\"\n",
        "  Corrects text using OpenAI's GPT-4o-mini model.\n",
        "  \n",
        "  Args:\n",
        "  text (str): The text to correct.\n",
        "  filename (str): The name of the file.\n",
        "  just_text (bool): Whether to return just the corrected text.\n",
        "  max_completion_tokens (int): The maximum number of tokens to generate.\n",
        "  temperature (float): The temperature for sampling.\n",
        "  top_p (float): The nucleus sampling probability.\n",
        "  frequency_penalty (float): The frequency penalty.\n",
        "  presence_penalty (float): The presence penalty.\n",
        "  kwargs: Additional keyword arguments.\n",
        "  \n",
        "  Returns:\n",
        "  str: The corrected text.\n",
        "  \"\"\"\n",
        "  response = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[\n",
        "      {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "          {\n",
        "            \"type\": \"text\",\n",
        "            \"text\": f\"Eres un experto en documentos históricos de Puerto Rico. El texto en español son noticias del siglo XX y contiene muchos errores a causa del OCR. Descifra el contenido y tradúcelo al inglés:\\n1. Preserva nombres propios (ej: Mayagüez, Caguas)\\n2. Ignora el \\\"header\\\" (ej:\\n```EL MUNDO\\nPRONOSTICOS DEL TIEMPO PARA LA ISLA, HOY: Mayormente nublado, con aguaceros dispersos temprano en la mafiana. EN SAN JUAN. AYER: Temperatura máxima. 80; mínima, 77. Presión barométrica al nivel del mar, a las 4:80 de la tarde. 38.88 pulgadas de mercurio. No hay indicios de disturbio tropical.\\n40 páginas 5/\\nDIARIO DE LA MARANA\\nAÑO XXVIII\\nEntered aa second clsss matter, Post Office, San Juan, P. R.)```\\n3. Ignora los anuncios\\n4. Solo mantén contenido relacionado a Puerto Rico (especialmente sobre ciudades, locaciones o eventos históricos)\\n5. Traduce el texto a inglés. Solo mantén los datos mas importantes\\n6.  Lista las ciudades o locaciones de Puerto Rico mencionadas\\n7. Escribe solo en texto (no uses **negrillas** ni *itálicas* ni nada en markdown)\\n8. return it as a JSON object with two fields:\\n    - 'metadata': un diccionario con la siguiente informacion: 'filename' (nombre del articulo), 'date' (fecha del articulo), 'locations' (lista de las ciudades o locaciones de Puerto Rico mencionadas).\\n    - 'text': the corrected and summarized text in English.\\n8. No digas nada mas ni preguntes más. El nombre del articulo es {filename}. Usa el siguiente texto: {text}\"\n",
        "          }\n",
        "        ]\n",
        "      }\n",
        "    ],\n",
        "    response_format={\n",
        "      \"type\": \"json_object\"\n",
        "    },\n",
        "    temperature=temperature,\n",
        "    max_completion_tokens=max_completion_tokens,\n",
        "    top_p=top_p,\n",
        "    frequency_penalty=frequency_penalty,\n",
        "    presence_penalty=presence_penalty,\n",
        "    **kwargs\n",
        "  )\n",
        "  if just_text:\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "  return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jRwc-LuNBPzt"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "import pickle as pkl\n",
        "\n",
        "def save_progress(data, filename=\"all_docs.pkl\"):\n",
        "    \"\"\" Save the current state of data to Google Drive. \n",
        "    \n",
        "    Args:\n",
        "    data (dict): The data to save.\n",
        "    filename (str): The filename to save the data to.\n",
        "    \n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    save_path = os.path.join(OUTPUT_DIR, filename)\n",
        "\n",
        "    with open(save_path, 'wb') as f:\n",
        "        pkl.dump(data, f)\n",
        "\n",
        "    print(f\"Progress saved at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} to {save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "sgLrBamrBPzu"
      },
      "outputs": [],
      "source": [
        "PROGRESS_FILE = os.path.join(OUTPUT_DIR, \"processed_files.log\")\n",
        "\n",
        "def get_processed_files():\n",
        "    \"\"\"\n",
        "    Returns a set of processed files.\n",
        "    \n",
        "    Returns:\n",
        "    set: The set of processed files\n",
        "    \"\"\"\n",
        "    if os.path.exists(PROGRESS_FILE):\n",
        "        with open(PROGRESS_FILE, 'r') as f:\n",
        "            return set(f.read().splitlines())\n",
        "    return set()\n",
        "\n",
        "def update_progress(filename):\n",
        "    \"\"\"\n",
        "    Updates the progress file with the processed filename.\n",
        "    \n",
        "    Args:\n",
        "    filename (str): The filename to add to the progress file.\n",
        "    \n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    with open(PROGRESS_FILE, 'a') as f:\n",
        "        f.write(f\"{filename}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "55H29AJ_BPzv"
      },
      "outputs": [],
      "source": [
        "# ========== Processing Pipeline ==========\n",
        "import json\n",
        "import pickle as pkl\n",
        "from langchain.docstore.document import Document\n",
        "import time\n",
        "\n",
        "# Save progress every 15 minutes\n",
        "interval_minutes = 15\n",
        "\n",
        "def process_files():\n",
        "    \"\"\"\n",
        "    Processes the text files in the ZIP archive.\n",
        "    \n",
        "    Returns:\n",
        "    list: A list of Document objects.\n",
        "    \"\"\"\n",
        "    extract_files(ZIP_PATH, EXTRACT_DIR)  # extract files from zip\n",
        "\n",
        "    all_docs = [] # for storing all the documents\n",
        "\n",
        "    # Track when the last save occurred\n",
        "    last_save_time = time.time()\n",
        "    processed = get_processed_files()\n",
        "\n",
        "    # Get all text files from nested directory\n",
        "    base_dir = os.path.join(EXTRACT_DIR, \"elmundo_chunked_es_page1_40years\")\n",
        "    txt_files = [f for f in os.listdir(base_dir) if f.endswith('.txt')]\n",
        "\n",
        "    for filename in tqdm(txt_files, desc=\"Processing files\"):\n",
        "\n",
        "        if filename in processed:\n",
        "            # Skip already processed files\n",
        "            continue\n",
        "\n",
        "        input_path = os.path.join(base_dir, filename)\n",
        "        output_path = os.path.join(OUTPUT_DIR, f\"cleaned_{filename}\")\n",
        "\n",
        "        with open(input_path, 'r', encoding='utf-8', errors='ignore') as f: # open current text file\n",
        "            raw_text = f.read()\n",
        "\n",
        "        try:\n",
        "            # gets gpt-4o-mini JSON object with 'metadata' and 'text' fields:\n",
        "            json_object = json.loads(correct_with_openai(raw_text, filename))  # OpenAI version\n",
        "\n",
        "            cleaned_text = json_object['text']  # get the text from the gpt-4o-mini model\n",
        "\n",
        "            with open(output_path, 'w', encoding='utf-8') as f: # save text on google drive\n",
        "                f.write(cleaned_text)\n",
        "\n",
        "            print(f\"Processed: {filename} -> Saved to Drive\")\n",
        "\n",
        "            doc = Document(                           # convert text to a langchain text object (for use on Chroma later)\n",
        "                page_content=json_object['text'],\n",
        "                metadata=json_object['metadata']\n",
        "            )\n",
        "            all_docs.append(doc)                      # append docs to list\n",
        "\n",
        "            # Update the processed log\n",
        "            update_progress(filename)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {filename}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "        current_time = time.time()\n",
        "        if (current_time - last_save_time) >= (interval_minutes * 60):\n",
        "            save_progress(all_docs)\n",
        "            last_save_time = current_time  # Update the last save time\n",
        "\n",
        "    # Save all_docs as pkl file\n",
        "    with open(os.path.join(OUTPUT_DIR, \"all_docs.pkl\"), 'wb') as f:\n",
        "        pkl.dump(all_docs, f)\n",
        "\n",
        "    with open(\"./saves/all_docs.pkl\", 'wb') as f:\n",
        "        pkl.dump(all_docs, f)\n",
        "\n",
        "    return all_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4MMyhLCBPzw",
        "outputId": "44f2cff0-b61b-4680-a45b-10625a937ea8"
      },
      "outputs": [],
      "source": [
        "# Run the pipeline. False by default, set to True to run in the first cell\n",
        "if RUN_PIPELINES:\n",
        "    all_docs = process_files()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def translate_list (lista, just_text = True, max_completion_tokens = 2048, temperature = 1, top_p = 1, frequency_penalty=0, presence_penalty=0,**kwargs):\n",
        "  \"\"\"\n",
        "  Translates a list of items to English using OpenAI's GPT-4o-mini model.\n",
        "  \n",
        "  Args:\n",
        "  lista (list): The list to translate.\n",
        "  just_text (bool): Whether to return just the translated text.\n",
        "  \n",
        "  Returns:\n",
        "  str: The translated text in a JSON-ish style.\n",
        "  \"\"\"\n",
        "  largo = len(lista)\n",
        "\n",
        "  if isinstance(lista, list):\n",
        "    lista = str(lista)\n",
        "\n",
        "  response = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[\n",
        "      {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "          {\n",
        "            \"type\": \"text\",\n",
        "            \"text\": f\"\"\"\n",
        "                Eres un experto en documentos históricos, localidades, y personas ilustres de Puerto Rico. La lista que se te pasará es un de lugares conocidos y \n",
        "                personas ilustres de Puerto Rico. Tu deber es traducir la lista al inglés y corregir cualquier error que encuentres. \n",
        "                1. Preserva nombres propios (ej: Mayagüez, Caguas, Julia de Burgos)\\n\n",
        "                2. Escribe solo en texto (no uses **negrillas** ni *itálicas* ni nada en markdown)\\n\n",
        "                3. Es posible que la lista ya contenga elementos en inglés. En ese caso, no los traduzcas, pero incluyelos en la respuesta final.\\n\n",
        "                3. Retorna un objeto JSON con {largo} pares key-value:\\n\n",
        "                    - key: el texto de la lista. Podría estar en inglés o español. Depende de como te lo dieron en la lista\\n\n",
        "                    - value: el texto traducido al inglés. Si el elemento ya estaba en inglés, no hace falta traducirlo, pero igualmente incluyes el texto aqui\\n\n",
        "                4. El ejemplo de como se veria la respuesta JSON aceptable para la lista de ejemplo [\"playa de Camuy\", \"Julia de Burgos\", \"Parque acuatico Las Cascadas\", \"Aguada Transmission Center\", \"Domes Beach\"]:\\n\n",
        "                    ```\n",
        "                    {{\n",
        "                        \"playa de Camuy\": \"Camuy Beach\",\n",
        "                        \"Julia de Burgos\": \"Julia de Burgos\",\n",
        "                        \"Parque acuático Las Cascadas\": \"Las Cascadas Water Park\",\n",
        "                        \"Centro Ceremonial Indígena de Caguana\": \"Caguana Indigenous Ceremonial Center\",\n",
        "                        \"Aguada Transmission Center\": \"Aguada Transmission Center\",\n",
        "                        \"Domes beach\": \"Domes beach\"\n",
        "                    }}\n",
        "                    ```\n",
        "                  En general, el JSON deberia verse {{key_1: value_1, key_2: value_2, key_3: value_3, ..., key_{largo}: value_{largo}}}\\n\n",
        "                5. La lista de arriba solamente es un ejemplo para que te guies.\n",
        "                6. Absolutamente todos los elementos en la que el usuario te de tienen que aparecer en el JSON final con su traducción correspondiente.\n",
        "                7. No digas nada mas ni preguntes más.\n",
        "                8. La lista del usuario que vas a usar para el JSON es la siguiente:\\n \n",
        "                    {lista}\n",
        "                \"\"\"\n",
        "          }\n",
        "        ]\n",
        "      }\n",
        "    ],\n",
        "    response_format={\n",
        "      \"type\": \"json_object\"\n",
        "    },\n",
        "    temperature=temperature,\n",
        "    max_completion_tokens=max_completion_tokens,\n",
        "    top_p=top_p,\n",
        "    frequency_penalty=frequency_penalty,\n",
        "    presence_penalty=presence_penalty,\n",
        "    **kwargs\n",
        "  )\n",
        "  if just_text:\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "  return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def update_metadata_with_landmarks(all_docs, landmarks_dict):\n",
        "    \"\"\"\n",
        "    Updates the metadata of the documents with the landmarks found in the text.\n",
        "    \n",
        "    Args:\n",
        "    all_docs (list): A list of Document objects.\n",
        "    landmarks_dict (dict): A dictionary of landmarks in English and Spanish.\n",
        "    \n",
        "    Returns:\n",
        "    list: A list of Document objects with updated metadata.\n",
        "    \"\"\"\n",
        "    for doc in all_docs:\n",
        "        text = doc.page_content.lower()  # Convert to lowercase for easier matching\n",
        "        for landmark_es, landmark_en in landmarks_dict.items():\n",
        "            if landmark_es.lower() in text or landmark_en.lower() in text:\n",
        "                if 'locations' not in doc.metadata:\n",
        "                    doc.metadata['locations'] = []\n",
        "                if landmark_en not in doc.metadata['locations']:\n",
        "                    doc.metadata['locations'].append(landmark_en)  # Add the English landmark\n",
        "                if landmark_es not in doc.metadata['locations']:\n",
        "                    doc.metadata['locations'].append(landmark_es)  # Add the Spanish landmark\n",
        "\n",
        "    return all_docs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "# Load spaCy's NER model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Example function to process the documents and add NER results to metadata\n",
        "def enrich_metadata_with_ner(all_docs):\n",
        "    \"\"\"\n",
        "    Enriches the metadata of the documents with named entities recognized by spaCy.\n",
        "    \n",
        "    Args:\n",
        "    all_docs (list): A list of Document objects.\n",
        "    \n",
        "    Returns:\n",
        "    list: A list of Document objects with updated metadata.\n",
        "    \"\"\"\n",
        "    for doc in all_docs:\n",
        "        text = doc.page_content\n",
        "        spacy_doc = nlp(text)  # Process text through spaCy NER engine\n",
        "\n",
        "        # Collect the detected locations (GPE and LOC entities)\n",
        "        ner_locations = {ent.text for ent in spacy_doc.ents if ent.label_ in ['GPE', 'LOC']}\n",
        "        \n",
        "        # Combine with existing locations in metadata\n",
        "        existing_locations = set(doc.metadata.get('locations', []))\n",
        "        updated_locations = list(existing_locations.union(ner_locations))\n",
        "        \n",
        "        # Update the document's metadata with enriched locations\n",
        "        doc.metadata['locations'] = ', '.join(updated_locations)\n",
        "    \n",
        "    return all_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_landmarks_dict():\n",
        "    \"\"\"\n",
        "    Returns a dictionary of landmarks in English and Spanish.\n",
        "    \n",
        "    Returns:\n",
        "    dict: A dictionary of landmarks in English and Spanish.\n",
        "    \"\"\"\n",
        "\n",
        "    path_zip = \"./data/landmarks.zip\"\n",
        "    extract = './tmp/extracted_landmarks'  # Using tmp for faster I/O\n",
        "    # Create directories\n",
        "    os.makedirs(extract, exist_ok=True)\n",
        "\n",
        "    # Extract the landmarks.zip file\n",
        "    extract_files(path_zip, extract)\n",
        "\n",
        "\n",
        "    #landmarks list of file names, removing .txt and changing `_`, and `-` to spaces\n",
        "    base_dir = os.path.join(extract, \"landmarks\")\n",
        "    landmarks = [f.replace('.txt', '').replace('_', ' ').replace('-', ' ') for f in os.listdir(base_dir) if f.endswith('.txt')]\n",
        "\n",
        "    # Translate the landmarks list with OpenAI (splitting into two to avoid exceeding the token limit)\n",
        "    translations1 = translate_list(landmarks[:len(landmarks)//2], just_text=False, max_completion_tokens=7000)\n",
        "    translations2 = translate_list(landmarks[len(landmarks)//2:], just_text=False, max_completion_tokens=7000)\n",
        "\n",
        "    # Combine the translations. translations contain 2 ChatCompletion objects\n",
        "    translations = [translations1, translations2]\n",
        "\n",
        "    #save translations to pkl\n",
        "    with open(\"./save/landmark translations.pkl\", 'wb') as f:\n",
        "        pkl.dump(translations, f)\n",
        "\n",
        "    # change the translations to json\n",
        "    translation1_json = json.loads(translations[0].choices[0].message.content)\n",
        "    translation2_json = json.loads(translations[1].choices[0].message.content)\n",
        "\n",
        "    # make translations_json by adding translation1_json and translation2_json\n",
        "    translations_json = {**translation1_json, **translation2_json}\n",
        "\n",
        "    # save translations_json to json file\n",
        "    with open(\"./saves/landmarks.json\", 'w') as f:\n",
        "        json.dump(translations_json, f)\n",
        "\n",
        "    return translations_json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# delete the extracted files\n",
        "! rm -rf ./tmp/extracted_landmarks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the pipeline. False by default, set to True to run in the first cell\n",
        "if RUN_PIPELINES:\n",
        "    translations = get_landmarks_dict()\n",
        "    \n",
        "    # update metadata with landmarks\n",
        "    all_docs = update_metadata_with_landmarks(all_docs, translations)\n",
        "    all_docs = enrich_metadata_with_ner(all_docs)\n",
        "    # save the updated all_docs to pkl\n",
        "    # add a source tag to the metadata with value 'news'\n",
        "    for doc in all_docs:\n",
        "        doc.metadata['source'] = 'news'\n",
        "        \n",
        "    with open(\"./saves/all_docs_updated.pkl\", 'wb') as f:\n",
        "        pkl.dump(all_docs, f)\n",
        "\n",
        "else:\n",
        "    with open(\"./saves/all_docs_updated.pkl\", 'rb') as f:\n",
        "        news_docs = pkl.load(f)      # load the updated all_docs from pkl file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Converting other articles to Documents and store them in an array for later using them for the ChromaDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# open csv files with pandas\n",
        "import pandas as pd\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "if RUN_PIPELINES:\n",
        "    landmarks = pd.read_csv(\"./structured-information-from-datasets/landmark_data_combined.csv\")\n",
        "    landmarks.head()\n",
        "\n",
        "    # convert save landmarks csv in a Documents object in a list\n",
        "    landmarks_docs = []\n",
        "    for i, row in landmarks.iterrows():\n",
        "        doc = Document(\n",
        "            page_content=row['Brief Description'],\n",
        "            metadata={\n",
        "                'filename': row['File Name'],\n",
        "                'landmark': row['Landmark Name'],\n",
        "                'latitude': row['Latitude'],\n",
        "                'longitude': row['Longitude'],\n",
        "                'municipality': row['Municipality'],\n",
        "                'url': row['Wikipedia URL'],\n",
        "                'source': 'landmarks'\n",
        "            }\n",
        "        )\n",
        "        landmarks_docs.append(doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "#open municipality csv file\n",
        "if RUN_PIPELINES:\n",
        "    municipalities = pd.read_csv(\"./structured-information-from-datasets/municipality_data_combined.csv\")\n",
        "    municipalities.head()\n",
        "\n",
        "    # convert save municipalities csv in a Documents object in a list\n",
        "    municipalities_docs = []\n",
        "    for i, row in municipalities.iterrows():\n",
        "        doc = Document(\n",
        "            page_content=row['Brief Description'],\n",
        "            metadata={\n",
        "                'filename': row['File Name'],\n",
        "                'municipality': row['Municipality Name'],\n",
        "                'latitude': row['Latitude'],\n",
        "                'longitude': row['Longitude'],\n",
        "                'url': row['Wikipedia URL'],\n",
        "                'source': 'municipalities'\n",
        "            }\n",
        "        )\n",
        "        municipalities_docs.append(doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Merge all lists of documents into one list and save them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# merge the news_docs, landmarks_docs, and municipalities_docs in a single list\n",
        "if RUN_PIPELINES:\n",
        "    all_docs = news_docs + landmarks_docs + municipalities_docs\n",
        "\n",
        "    # save the merged all_docs to pkl\n",
        "    with open(\"./saves/news_landmarks_municipalities_merged.pkl\", 'wb') as f:\n",
        "        pkl.dump(all_docs, f)\n",
        "\n",
        "else:\n",
        "    with open(\"./saves/news_landmarks_municipalities_merged.pkl\", 'rb') as f:\n",
        "        all_docs = pkl.load(f)      # load the updated all_docs from pkl file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ChromaDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\Natanael\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n",
            "Initialized SentenceTransformer embeddings.\n",
            "\n",
            "Loading database...\n",
            "Huggingface database loaded.\n"
          ]
        }
      ],
      "source": [
        "## ========== Chroma ==========\n",
        "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "create_db = False\n",
        "\n",
        "if create_db:\n",
        "    sentence_transformer_embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "    print(\"Initialized SentenceTransformer embeddings.\")\n",
        "\n",
        "    # Load all documents into Chroma\n",
        "    db = Chroma.from_documents(all_docs, sentence_transformer_embeddings, persist_directory=\"./chroma_db\")\n",
        "    print('All documents loaded and embedded.(huggingface)')\n",
        "\n",
        "else:\n",
        "    sentence_transformer_embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "    print(\"Initialized SentenceTransformer embeddings.\")\n",
        "\n",
        "    print(\"\\nLoading database...\")\n",
        "    db = Chroma(persist_directory='./chroma_db', embedding_function=sentence_transformer_embeddings)\n",
        "    print(\"Huggingface database loaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Natanael\\AppData\\Local\\Temp\\ipykernel_16392\\2304997646.py:51: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
            "  llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7, openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
            "C:\\Users\\Natanael\\AppData\\Local\\Temp\\ipykernel_16392\\2304997646.py:64: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = qa_chain.run(f\"How does {loc_name} match the user's preference: {user_prompt}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Esperanza Beach: Esperanza Beach might not fully match your preference for historical places, as it is mainly known for being a popular beach destination with hotels, restaurants, and shops. However, it does offer a sunny beach experience with its proximity to La Esperanza and the beautiful coastline. If you are specifically looking for historical places, you may want to explore other areas in Puerto Rico that are known for their historical significance.\n",
            "La Pocita de las Golondrinas Beach: La Pocita de las Golondrinas Beach in Isabela is a sunny beach that is safe for families with children due to its shallow waters, but it does not have historical places nearby.\n",
            "Jobos Beach: Jobos Beach may not be the best match for the user's preference of loving sunny beaches and historical places. While Jobos Beach is a sunny beach located in Puerto Rico, it does not have significant historical landmarks or sites nearby. If historical places are an important factor for the user, they may want to consider other beaches in Puerto Rico such as those in Vieques which have historical landmarks like the Esperanza Pier.\n",
            "Buyé Beach: Buyé Beach in Cabo Rojo, Puerto Rico, may not match your preference for historical places as it is primarily known for its sunny beach with golden sands and clear waters. While it is a beautiful spot for sunbathing and swimming, it doesn't have significant historical sites or landmarks nearby. If historical places are a key factor in your preference, you may want to consider other beaches in Puerto Rico, like those in Old San Juan, which offer a combination of historical sites and beautiful beaches.\n",
            "Crash Boat Beach: Crash Boat Beach in Puerto Rico is a sunny beach destination; however, it may not directly align with your preference for historical places as it is primarily known for its beautiful sandy shores and not for historical significance. If you are specifically looking for a combination of sunny beaches and historical places, you may want to explore other locations in Puerto Rico that offer both elements.\n",
            "Crash Boat Beach: Crash Boat Beach in Aguadilla, Puerto Rico, is a sunny beach that might match your preference as it is known for its sunny weather and beautiful coastline. However, in terms of historical places, Crash Boat Beach itself does not have significant historical sites directly on the beach. If you are specifically looking for historical places alongside the sunny beach, you might want to consider visiting other historical sites in Aguadilla or nearby areas in Puerto Rico.\n",
            "San Juan Marriott Resort &amp; Stellaris Casino: The San Juan Marriott Resort & Stellaris Casino is located on the beach in Condado, San Juan, Puerto Rico, which means it offers a sunny beach experience. However, it may not fully meet the preference for historical places as it is primarily a modern hotel and casino. If you are specifically looking for historical places, you might want to explore other historical sites in Puerto Rico.\n",
            "None: None of the mentioned beaches (Flamenco Beach, Esperanza Beach, Crash Boat Beach, La Pocita de las Golondrinas Beach, Caracas Beach) specifically match the user's preference for loving sunny beaches and historical places. These beaches are known for their natural beauty, clear waters, and recreational activities rather than historical significance.\n",
            "Playa Espinar: Playa Espinar in Aguada, Puerto Rico, is a sunny beach on the north-west coast, but it may not directly match your preference for historical places. It seems to be more focused on being a beach destination rather than having significant historical sites nearby.\n",
            "Caracas Beach (Vieques): Caracas Beach in Vieques may not directly match your preference for historical places, as it is primarily known for its white sand, clear blue waters, and snorkeling opportunities. However, the beach does offer a trail that leads to a nearby hill providing beautiful views of the beach and cove, which might offer some historical context or cultural insights. So, while it may not be a historical site per se, the trail and the surrounding area could provide some historical elements to enhance your beach experience.\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "def rank_appropriate_locations(user_prompt):\n",
        "    \"\"\"\n",
        "    This function ranks appropriate locations for the user based on their preferences in the prompt.\n",
        "    It uses Chroma for document retrieval and RAG for ranking locations based on context-aware responses.\n",
        "\n",
        "    Parameters:\n",
        "    - user_prompt (str): The user's question or preferences (e.g., \"I love sunny beaches\").\n",
        "\n",
        "    Returns:\n",
        "    - list: A ranked list of location suggestions based on user preferences.\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Analyze the user prompt for keywords (e.g., \"beach,\" \"history,\" \"sunny\")\n",
        "    preferences = user_prompt.lower()\n",
        "    keyword_list = ['beach', 'sunny', 'history', 'museum', 'nature', 'mountain', 'culture']  # Example keyword list\n",
        "\n",
        "    # Step 2: Query Chroma to retrieve locations that match user preferences\n",
        "    relevant_locations = []\n",
        "    relevance_scores = {}  # Store relevance score for each location\n",
        "\n",
        "    for keyword in keyword_list:\n",
        "        if keyword in preferences:\n",
        "            query = f\"{keyword} locations\"\n",
        "            try:\n",
        "                retrieved_docs = db.similarity_search(query, k=5)  # Adjust 'k' for the number of retrieved documents\n",
        "                if retrieved_docs:\n",
        "                    for doc in retrieved_docs:\n",
        "                        loc_name = doc.metadata.get('landmark')\n",
        "                        page_content = doc.page_content.lower()\n",
        "                        # Assign a relevance score based on the presence of the keyword in the content\n",
        "                        score = page_content.count(keyword)  # Count how many times the keyword appears in the content\n",
        "                        if loc_name not in relevance_scores:\n",
        "                            relevance_scores[loc_name] = score\n",
        "                        else:\n",
        "                            relevance_scores[loc_name] += score\n",
        "                    relevant_locations.extend(retrieved_docs)\n",
        "            except Exception as e:\n",
        "                return f\"Error: Failed to retrieve documents from Chroma. Please try again later. Error details: {str(e)}\"\n",
        "\n",
        "    # Edge case: If no relevant documents are retrieved\n",
        "    if not relevant_locations:\n",
        "        return \"Sorry, no relevant locations were found based on your preferences. Please try a different query.\"\n",
        "\n",
        "    # Step 3: Rank the retrieved locations using RAG\n",
        "    ranked_locations = []\n",
        "    try:\n",
        "        # Initialize the LLM (Large Language Model) for RAG\n",
        "        llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7, openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "        # Create the RetrievalQA chain using Chroma vector store and the LLM\n",
        "        qa_chain = RetrievalQA.from_chain_type(\n",
        "            llm=llm,\n",
        "            chain_type=\"stuff\",  # Using 'stuff' to combine document contents into one response\n",
        "            retriever=db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})  # Retrieve top 5 documents\n",
        "        )\n",
        "\n",
        "        # Step 4: For each retrieved location, generate a response based on user preferences\n",
        "        for location in relevant_locations:\n",
        "            loc_name = location.metadata.get('landmark')\n",
        "            # Pass the location to RAG with the user prompt\n",
        "            response = qa_chain.run(f\"How does {loc_name} match the user's preference: {user_prompt}\")\n",
        "            ranked_locations.append((loc_name, response, relevance_scores.get(loc_name, 0)))  # Store location with response and score\n",
        "\n",
        "        # Step 5: Sort locations based on relevance score (higher score is better)\n",
        "        ranked_locations.sort(key=lambda x: x[2], reverse=True)  # Sort by relevance score\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error: Failed to rank locations using RAG. Please try again later. Error details: {str(e)}\"\n",
        "\n",
        "    # Step 6: Return the ranked list of location suggestions\n",
        "    ranked_list = \"\\n\".join([f\"{loc[0]}: {loc[1]}\" for loc in ranked_locations])  # Format the output\n",
        "    return ranked_list\n",
        "\n",
        "# Example usage: Rank locations based on a user prompt\n",
        "user_prompt = \"I love sunny beaches and historical places.\"\n",
        "ranked_locations = rank_appropriate_locations(user_prompt)\n",
        "print(ranked_locations)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define extract_interests function\n",
        "def extract_interests(user_prompt):\n",
        "    \"\"\"\n",
        "    A simple function to extract potential interests from the user's prompt.\n",
        "    This could be expanded to use NLP techniques to identify more complex patterns.\n",
        "    \"\"\"\n",
        "    interests = []\n",
        "\n",
        "    # Example keywords associated with different types of preferences\n",
        "    keywords = {\n",
        "        'sunny': ['sunny', 'beach', 'warm', 'hot', 'tropical'],\n",
        "        'history': ['history', 'museum', 'historic', 'culture', 'ancient'],\n",
        "        'nature': ['nature', 'outdoor', 'park', 'mountain', 'trail'],\n",
        "        'rain': ['rainy', 'wet', 'storm']\n",
        "    }\n",
        "\n",
        "    # Check for keywords in the user prompt\n",
        "    for category, words in keywords.items():\n",
        "        if any(word in user_prompt.lower() for word in words):\n",
        "            interests.append(category)\n",
        "\n",
        "    return interests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted Interests: ['sunny']\n",
            "Search Results: [Document(id='5846625d-652b-4789-8ead-4f6210738e3a', metadata={'filename': 'flamenco_beach.txt', 'landmark': 'Flamenco Beach', 'latitude': 18.331667, 'longitude': -65.318056, 'municipality': 'Culebra', 'source': 'landmarks', 'url': 'https://en.wikipedia.org/wiki/Flamenco_Beach'}, page_content=\"Flamenco Beachis a publicbeachon the Caribbeanisland of Culebra. It is known for its shallow turquoise waters, white sand, swimming areas, and diving sites. It stretches for a mile around a sheltered, horseshoe-shaped bay. Playa Flamenco is a popular beach destination for both Culebra and Puerto Rico. In March 2014, Flamenco Beach was ranked 3rd best beach in the world with a Trip Advisor Travelers\\\\' Choice Award. [1][2]In August 2016, it was announced that Flamenco Beach had regained its Blue Flag Beachinternational distinction. [3]\\\\n\"), Document(id='b6fe09b9-0db2-404c-b29c-fce819294110', metadata={'filename': 'jobos_beach.txt', 'landmark': 'Jobos Beach', 'latitude': 18.514215, 'longitude': -67.075744, 'municipality': 'Isabela', 'source': 'landmarks', 'url': 'https://en.wikipedia.org/wiki/Jobos_Beach'}, page_content='Jobos Beachor Playa Jobosis abeachfacing the Atlantic Oceanlocated on the PR-466street of Isabelain the northwest of Puerto Rico. [1]While popular, the beach is dangerous and signs warning tourists of dangerous currents were installed in early 2021. [2]\\\\n'), Document(id='b4fa74ac-011c-486c-bd0d-e0dc8f4eab41', metadata={'date': 'March 15, 1947', 'filename': '19470315_1.txt', 'locations': 'World War II, Puerto Rico, Culebra, puerto rico, San Juan, San Germán, world war ii', 'source': 'news'}, page_content='Partly cloudy today in Puerto Rico, with light scattered showers in the eastern part of the island, ending early in the morning. Winds from the east at 11 to 20 miles per hour, decreasing well into the afternoon. Yesterday in San Juan, the maximum temperature was 78°F and the minimum was 71°F.\\n\\nIn San Germán, a tragic family incident occurred when a father armed with a pick handle attacked his wife and children. One daughter died, and afterward, the assailant committed suicide.\\n\\nIn Culebra, an amphibious operation involving five thousand marines took place successfully, marking the first amphibious exercise since World War II, with no serious accidents reported.'), Document(id='ff9fbce2-ede9-48b1-a2c3-e51feb7189da', metadata={'filename': 'blue_beach_(vieques).txt', 'landmark': 'Blue Beach (Vieques)', 'latitude': 18.11305555555556, 'longitude': -65.3875, 'municipality': 'Vieques', 'source': 'landmarks', 'url': 'https://en.wikipedia.org/wiki/Blue_Beach_(Vieques)'}, page_content='Blue Beach, better known as La Chiva Beach, is a popular beach on the southern coast of Viequesin barrio Puerto Ferro. The beach is considered one of the best in both Vieques and Puerto Ricofor its breathtaking scenery, its soft sand and crystal clear water, ideal for swimming and snorkeling. [1][2][3]The beach is known as La Chiva Beach by locals, and its Englishname originated as a nickname that was given to it by members of the US Navy. [4]The area was formerly used by the Navy but it has been declared safe for visitors; however, beachgoers are prohibited from entering La Chiva Key due to the risk ofunexploded ordnance. [4]The beach was closed for a time after the destruction caused by Hurricane Irmain 2017 but it reopened the next year. [5]\\\\n'), Document(id='1b465b87-a6f7-49ac-8011-39ee40b4ec18', metadata={'filename': 'playa_espinar.txt', 'landmark': 'Playa Espinar', 'latitude': 18.3916128, 'longitude': -67.1962921, 'municipality': 'Aguada', 'source': 'landmarks', 'url': 'https://en.wikipedia.org/wiki/Playa_Espinar'}, page_content='18°23\\\\xe2\\\\x80\\\\xb230\\\\xe2\\\\x80\\\\xb3N67°11\\\\xe2\\\\x80\\\\xb247\\\\xe2\\\\x80\\\\xb3W\\ufeff / \\ufeff18. 3916128°N 67. 1962921°W\\ufeff /18. 3916128; -67. 1962921\\\\n Playa Espinaris a beach on the north-west coast of Puerto Rico, in the town of Aguada. The beach is 760 meters long. [1]\\\\n')]\n",
            "Unique Search Results: [Document(id='5846625d-652b-4789-8ead-4f6210738e3a', metadata={'filename': 'flamenco_beach.txt', 'landmark': 'Flamenco Beach', 'latitude': 18.331667, 'longitude': -65.318056, 'municipality': 'Culebra', 'source': 'landmarks', 'url': 'https://en.wikipedia.org/wiki/Flamenco_Beach'}, page_content=\"Flamenco Beachis a publicbeachon the Caribbeanisland of Culebra. It is known for its shallow turquoise waters, white sand, swimming areas, and diving sites. It stretches for a mile around a sheltered, horseshoe-shaped bay. Playa Flamenco is a popular beach destination for both Culebra and Puerto Rico. In March 2014, Flamenco Beach was ranked 3rd best beach in the world with a Trip Advisor Travelers\\\\' Choice Award. [1][2]In August 2016, it was announced that Flamenco Beach had regained its Blue Flag Beachinternational distinction. [3]\\\\n\"), Document(id='b6fe09b9-0db2-404c-b29c-fce819294110', metadata={'filename': 'jobos_beach.txt', 'landmark': 'Jobos Beach', 'latitude': 18.514215, 'longitude': -67.075744, 'municipality': 'Isabela', 'source': 'landmarks', 'url': 'https://en.wikipedia.org/wiki/Jobos_Beach'}, page_content='Jobos Beachor Playa Jobosis abeachfacing the Atlantic Oceanlocated on the PR-466street of Isabelain the northwest of Puerto Rico. [1]While popular, the beach is dangerous and signs warning tourists of dangerous currents were installed in early 2021. [2]\\\\n'), Document(id='b4fa74ac-011c-486c-bd0d-e0dc8f4eab41', metadata={'date': 'March 15, 1947', 'filename': '19470315_1.txt', 'locations': 'World War II, Puerto Rico, Culebra, puerto rico, San Juan, San Germán, world war ii', 'source': 'news'}, page_content='Partly cloudy today in Puerto Rico, with light scattered showers in the eastern part of the island, ending early in the morning. Winds from the east at 11 to 20 miles per hour, decreasing well into the afternoon. Yesterday in San Juan, the maximum temperature was 78°F and the minimum was 71°F.\\n\\nIn San Germán, a tragic family incident occurred when a father armed with a pick handle attacked his wife and children. One daughter died, and afterward, the assailant committed suicide.\\n\\nIn Culebra, an amphibious operation involving five thousand marines took place successfully, marking the first amphibious exercise since World War II, with no serious accidents reported.'), Document(id='ff9fbce2-ede9-48b1-a2c3-e51feb7189da', metadata={'filename': 'blue_beach_(vieques).txt', 'landmark': 'Blue Beach (Vieques)', 'latitude': 18.11305555555556, 'longitude': -65.3875, 'municipality': 'Vieques', 'source': 'landmarks', 'url': 'https://en.wikipedia.org/wiki/Blue_Beach_(Vieques)'}, page_content='Blue Beach, better known as La Chiva Beach, is a popular beach on the southern coast of Viequesin barrio Puerto Ferro. The beach is considered one of the best in both Vieques and Puerto Ricofor its breathtaking scenery, its soft sand and crystal clear water, ideal for swimming and snorkeling. [1][2][3]The beach is known as La Chiva Beach by locals, and its Englishname originated as a nickname that was given to it by members of the US Navy. [4]The area was formerly used by the Navy but it has been declared safe for visitors; however, beachgoers are prohibited from entering La Chiva Key due to the risk ofunexploded ordnance. [4]The beach was closed for a time after the destruction caused by Hurricane Irmain 2017 but it reopened the next year. [5]\\\\n'), Document(id='1b465b87-a6f7-49ac-8011-39ee40b4ec18', metadata={'filename': 'playa_espinar.txt', 'landmark': 'Playa Espinar', 'latitude': 18.3916128, 'longitude': -67.1962921, 'municipality': 'Aguada', 'source': 'landmarks', 'url': 'https://en.wikipedia.org/wiki/Playa_Espinar'}, page_content='18°23\\\\xe2\\\\x80\\\\xb230\\\\xe2\\\\x80\\\\xb3N67°11\\\\xe2\\\\x80\\\\xb247\\\\xe2\\\\x80\\\\xb3W\\ufeff / \\ufeff18. 3916128°N 67. 1962921°W\\ufeff /18. 3916128; -67. 1962921\\\\n Playa Espinaris a beach on the north-west coast of Puerto Rico, in the town of Aguada. The beach is 760 meters long. [1]\\\\n')]\n",
            "Document Metadata: {'filename': 'flamenco_beach.txt', 'landmark': 'Flamenco Beach', 'latitude': 18.331667, 'longitude': -65.318056, 'municipality': 'Culebra', 'source': 'landmarks', 'url': 'https://en.wikipedia.org/wiki/Flamenco_Beach'}\n",
            "Document Metadata: {'filename': 'jobos_beach.txt', 'landmark': 'Jobos Beach', 'latitude': 18.514215, 'longitude': -67.075744, 'municipality': 'Isabela', 'source': 'landmarks', 'url': 'https://en.wikipedia.org/wiki/Jobos_Beach'}\n",
            "Document Metadata: {'date': 'March 15, 1947', 'filename': '19470315_1.txt', 'locations': 'World War II, Puerto Rico, Culebra, puerto rico, San Juan, San Germán, world war ii', 'source': 'news'}\n",
            "Document Metadata: {'filename': 'blue_beach_(vieques).txt', 'landmark': 'Blue Beach (Vieques)', 'latitude': 18.11305555555556, 'longitude': -65.3875, 'municipality': 'Vieques', 'source': 'landmarks', 'url': 'https://en.wikipedia.org/wiki/Blue_Beach_(Vieques)'}\n",
            "Document Metadata: {'filename': 'playa_espinar.txt', 'landmark': 'Playa Espinar', 'latitude': 18.3916128, 'longitude': -67.1962921, 'municipality': 'Aguada', 'source': 'landmarks', 'url': 'https://en.wikipedia.org/wiki/Playa_Espinar'}\n",
            "Ranked Locations: []\n",
            "No locations were ranked.\n"
          ]
        }
      ],
      "source": [
        "def rank_appropriate_locations(user_prompt, db):\n",
        "    \"\"\"\n",
        "    This function ranks locations based on the user's interests and the Chroma vector store.\n",
        "\n",
        "    Parameters:\n",
        "    - user_prompt (str): User's query or preferences (e.g., \"I love sunny beaches\").\n",
        "    - db (Chroma): Chroma vector store to query data from landmarks, municipalities, and news articles.\n",
        "\n",
        "    Returns:\n",
        "    - ranked_locations (list): List of locations ranked by relevance to the user's interests.\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Preprocess the user prompt to extract interests\n",
        "    interests = extract_interests(user_prompt)\n",
        "    print(f\"Extracted Interests: {interests}\")  # Debugging line to check the extracted interests\n",
        "\n",
        "    if not interests:\n",
        "        print(\"No interests extracted from the user prompt.\")  # If no interests are extracted\n",
        "\n",
        "    # Step 2: Search Chroma vector store for relevant locations\n",
        "    search_results = db.similarity_search(user_prompt, k=5)  # Adjust 'k' to retrieve more locations\n",
        "    print(f\"Search Results: {search_results}\")  # Debugging line to check Chroma search results\n",
        "\n",
        "    if not search_results:\n",
        "        print(\"No results found in Chroma for the given prompt.\")  # If no results were found\n",
        "\n",
        "    # Step 3: Remove duplicate documents from the search results based on metadata\n",
        "    unique_results = []\n",
        "    seen = set()\n",
        "    for doc in search_results:\n",
        "        metadata = doc.metadata\n",
        "        if metadata['filename'] not in seen:\n",
        "            seen.add(metadata['filename'])\n",
        "            unique_results.append(doc)\n",
        "\n",
        "    print(f\"Unique Search Results: {unique_results}\")  # Debugging line to check unique search results\n",
        "\n",
        "    # Step 4: Rank locations based on the interests\n",
        "    ranked_locations = []\n",
        "    for doc in unique_results:\n",
        "        metadata = doc.metadata\n",
        "        print(f\"Document Metadata: {metadata}\")  # Debugging line to check document metadata\n",
        "\n",
        "        score = 0\n",
        "        # Check if any of the interests match the document's page content (not just metadata)\n",
        "        for interest in interests:\n",
        "            # Check if interest matches the content of the document\n",
        "            if interest.lower() in doc.page_content.lower():\n",
        "                score += 1\n",
        "\n",
        "        # If the location has relevant scores, add it to the ranked list\n",
        "        if score > 0:\n",
        "            ranked_locations.append({\n",
        "                'location': metadata.get('landmark', metadata.get('municipality', 'Unknown')),\n",
        "                'score': score,\n",
        "                'metadata': metadata\n",
        "            })\n",
        "\n",
        "    # Step 5: Sort locations by score (higher score means more relevant)\n",
        "    ranked_locations = sorted(ranked_locations, key=lambda x: x['score'], reverse=True)\n",
        "    print(f\"Ranked Locations: {ranked_locations}\")  # Debugging line to check the final output\n",
        "\n",
        "    return ranked_locations\n",
        "\n",
        "# Example usage: Get ranked location suggestions based on user preferences\n",
        "user_prompt = \"I love sunny beaches and warm weather\"\n",
        "# Assuming db is your Chroma vector store object\n",
        "# Example: db = Chroma(persist_directory='path_to_your_chroma_db', embedding_function=embedding_function)\n",
        "suggestions = rank_appropriate_locations(user_prompt, db)\n",
        "\n",
        "# Display the suggestions\n",
        "if suggestions:\n",
        "    for suggestion in suggestions:\n",
        "        print(f\"Location: {suggestion['location']}\")\n",
        "        print(f\"Score: {suggestion['score']}\")\n",
        "        print(f\"Description: {suggestion['metadata'].get('description', 'No description available')}\")\n",
        "        print(f\"Wikipedia URL: {suggestion['metadata'].get('url', 'No URL available')}\")\n",
        "        print(\"-\" * 80)\n",
        "else:\n",
        "    print(\"No locations were ranked.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "The San Juan Marriott Resort & Stellaris Casinois ahotelandcasinolocated on the beach in Condado, San Juan, Puerto Rico. It is operated by Marriott International. \\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "landmarks\n",
            "******************************************************************************** \n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "Esperanza Beach is a popular beach on the southern coast of Viequesin La Esperanza, Puerto Real. In comparison to other beaches in the island which are located far away from populated areas, this beach is located close to La Esperanza and it hosts a number of hotels, restaurants, food kiosks and stores. [1][2]Esperanza Pier, located on the western part of the beach, is considered a landmark of Vieques. [3]It is a very popular weekend destination for locals and visitors alike. The beach is located between Sun Bay Beachand Black Sand Beachand both can be reached by foot from La Esperanza. [4][5][6]\\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "landmarks\n",
            "******************************************************************************** \n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "Mar Bella Beach, colloquially known as Puerto Nuevo Beach, is a beach in the municipality of Vega Bajain the north coast of Puerto Rico. The beach is often referred to as Puerto Nuevo Beach because it is located in the Puerto Nuevobarrio of Vega Baja; it is also referred to as the Balneario de Vega Bajaor Balneario de Puerto Nuevo. [1]The beach is located approximately 45 minutes west of San Juan, making it popular with both locals and visitors. [2]\\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "landmarks\n",
            "******************************************************************************** \n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "Crash Boat Beachor Crashboat Beach on the northwestern coast of Puerto Ricois situated in the municipality of Aguadilla. \\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "landmarks\n",
            "******************************************************************************** \n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "Cayo Luis Peña, formerly South West Key, [1]is a small uninhabited island off the west coast of Culebra, an island municipality of Puerto Rico. The island is anature reservewhich forms part of the Culebra National Wildlife Refuge. Visitors are allowed on the island for nature walks, snorkeling, andswimming; however, visitors are not allowed to stay on the island overnight. The island is only accessible via privatewater taxis. This limited access results in relatively few visitors and the island and surrounding reefs are able to stay more pristine as a result. [2]The small number of visitors also makes the island more private for those willing to make the journey. [3]Luis Peña Beach is located on the north side of the island. The island is named after its second owner. \\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "landmarks\n",
            "******************************************************************************** \n",
            "\n"
          ]
        }
      ],
      "source": [
        "#import markdown\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# Example query\n",
        "user_question = \"Beach in San Juan\"\n",
        "docs = db.similarity_search(user_question, k=5)\n",
        "\n",
        "# Print results\n",
        "for doc in docs[0:5]:\n",
        "    # print(doc.page_content, '\\n')\n",
        "    display(Markdown(doc.page_content))\n",
        "    print(doc.metadata['source'])\n",
        "    print(\"*\"*80, '\\n')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
