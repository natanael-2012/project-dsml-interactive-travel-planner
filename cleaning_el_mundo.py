# -*- coding: utf-8 -*-
"""cleaning el mundo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/natanael-2012/project-dsml-interactive-travel-planner/blob/main/cleaning%20el%20mundo.ipynb
"""

!pip install -q openai python-dotenv transformers torch tqdm accelerate

# from dotenv import load_dotenv
import os

from google.colab import userdata
HF_TOKEN = userdata.get('HF_TOKEN')
# userdata.get('OPENAI_API_KEY')

# load_dotenv()
# huggingface_token = os.getenv("HUGGINGFACE_TOKEN")
# openai_token = os.getenv("OPENAI_API_KEY")



from google.colab import drive
import os
import zipfile
from tqdm import tqdm
# from dotenv import load_dotenv
from openai import OpenAI
from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM

# ========== Mount Google Drive ==========
drive.mount('/content/drive')

# ========== Path Configuration ==========
# Update these paths according to your Google Drive structure
DRIVE_BASE = '/content/drive/MyDrive/'
ZIP_PATH = "./data/elmundo_chunked_es_page1_40years.zip"
EXTRACT_DIR = '/tmp/extracted'  # Using tmp for faster I/O
OUTPUT_DIR = os.path.join(DRIVE_BASE, 'cleaned_articles1')

# Create directories
os.makedirs(EXTRACT_DIR, exist_ok=True)
os.makedirs(OUTPUT_DIR, exist_ok=True)

# ========== File Extraction ==========
def extract_files():
    with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:
        # Extract nested structure
        for file in zip_ref.namelist():
            if file.endswith('.txt'):
                zip_ref.extract(file, EXTRACT_DIR)
                print(f"{file} extracted")

    print(f"Extracted files to: {EXTRACT_DIR}")

# ========== OpenAI Correction ==========
def correct_with_openai(text):
    client = OpenAI()

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {
                "role": "system",
                "content": """Eres un experto en documentos histÃ³ricos de Puerto Rico. Corrige errores OCR en espaÃ±ol:
1. Preserva nombres propios (ej: MayagÃ¼ez, Caguas)
2. Corrige errores de caracteres (Ã± â†’ n, fl â†’ Ã±)
3. MantÃ©n formato original de fechas (17 de enero de 1920)
4. Corrige palabras rotas (comuni caciÃ³n â†’ comunicaciÃ³n)"""
            },
            {
                "role": "user",
                "content": f"Texto a corregir:\n{text}"
            }
        ],
        temperature=0.1,
        max_tokens=2000
    )
    return response.choices[0].message.content

!pip install -q transformers==4.38.2 torch==2.2.1
!rm -rf ~/.cache/huggingface/hub  # Clear corrupted cache

from transformers import T5Tokenizer, T5ForConditionalGeneration
import requests

# ========== Verify Internet Access ==========
try:
    response = requests.get("https://huggingface.co/google/flan-t5-small", timeout=10)
    print("Hugging Face Hub is reachable âœ…")
except:
    print("NO INTERNET ACCESS TO HUGGINGFACE âŒ")
    # If using Colab: Runtime â†’ Disconnect & delete runtime â†’ Start fresh

try:
    tokenizer = T5Tokenizer.from_pretrained(
        "google/flan-t5-small",
        force_download=True,  # Bypass cache
        resume_download=False
    )
    model = T5ForConditionalGeneration.from_pretrained(
        "google/flan-t5-small",
        device_map="auto"
    )
    print("Model loaded successfully! ðŸŽ‰")
except Exception as e:
    print(f"Error: {str(e)}")
    # Fallback to local copy if available

# Commented out IPython magic to ensure Python compatibility.
!mkdir -p flan-t5-small
# %cd flan-t5-small

# Download all required files manually
!wget -q https://huggingface.co/google/flan-t5-small/resolve/main/config.json
!wget -q https://huggingface.co/google/flan-t5-small/resolve/main/pytorch_model.bin
!wget -q https://huggingface.co/google/flan-t5-small/resolve/main/special_tokens_map.json
!wget -q https://huggingface.co/google/flan-t5-small/resolve/main/spiece.model
!wget -q https://huggingface.co/google/flan-t5-small/resolve/main/tokenizer_config.json
!wget -q https://huggingface.co/google/flan-t5-small/resolve/main/generation_config.json

# %cd ..

from transformers import T5Tokenizer, T5ForConditionalGeneration

# Load from local directory
tokenizer = T5Tokenizer.from_pretrained("./flan-t5-small")
model = T5ForConditionalGeneration.from_pretrained("./flan-t5-small", device_map="auto")

def correct_with_opensource(text):
    inputs = tokenizer(
        f"Este texto es de un periodico llamado 'El Mundo' y contiene noticias de Puerto Rico del siglo XX. Corrige errores OCR en este texto espaÃ±ol manteniendo nombres propios y formato: {text}",
        return_tensors="pt",
        max_length=512,
        truncation=True
    ).to(model.device)

    outputs = model.generate(
        inputs.input_ids,
        max_length=1024,
        num_beams=3
    )
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# ========== Summarize Text ==========
sum_tokenizer = AutoTokenizer.from_pretrained("mrm8488/bert2bert_shared-spanish-finetuned-summarization")
sum_model = AutoModelForSeq2SeqLM.from_pretrained("mrm8488/bert2bert_shared-spanish-finetuned-summarization").to("cuda")

def summarize_text(text):
    inputs = sum_tokenizer(
        text,
        return_tensors="pt",
        max_length=512,
        truncation=True
    ).to("cuda")
    
    outputs = sum_model.generate(
        inputs.input_ids,
        max_length=512,
        min_length=256,
        num_beams=4,
        early_stopping=True
    )
    return sum_tokenizer.decode(outputs[0], skip_special_tokens=True)

# ========== Translation Pipeline ==========
translator = pipeline("translation_es_to_en", 
                     model="Helsinki-NLP/opus-mt-es-en",
                     device=0)

# ========== Chunk text ==========
splitter = SentenceSplitter(language='es')
def chunk_text(text, max_chars=1000):
    """Split text into meaningful chunks preserving sentence boundaries"""
    chunks = []
    current_chunk = []
    current_len = 0
    
    for sentence in splitter.split(text):
        sent_len = len(sentence)
        if current_len + sent_len > max_chars:
            chunks.append(" ".join(current_chunk))
            current_chunk = [sentence]
            current_len = sent_len
        else:
            current_chunk.append(sentence)
            current_len += sent_len
            
    if current_chunk:
        chunks.append(" ".join(current_chunk))
    return chunks




# ========== Processing Pipeline ==========
def process_files():
    extract_files()

    # Get all text files from nested directory
    base_dir = os.path.join(EXTRACT_DIR, "elmundo_chunked_es_page1_40years")
    txt_files = [f for f in os.listdir(base_dir) if f.endswith('.txt')]

    for filename in tqdm(txt_files, desc="Processing files"):
        input_path = os.path.join(base_dir, filename)
        output_path = os.path.join(OUTPUT_DIR, f"cleaned_{filename}")

        with open(input_path, 'r', encoding='utf-8', errors='ignore') as f:
            raw_text = f.read()

        try:
            # Choose one method:
            # cleaned_text = correct_with_openai(raw_text)  # OpenAI version
            cleaned_text = [correct_with_opensource(chunk) for chunk in chunk_text(raw_text)]  # Open-source version
            
            if len(cleaned_text.split()) > 750:  # Only summarize long articles
                summary = summarize_text(cleaned_text)
            else:
                summary = cleaned_text

            # 3. Translate
            translated = translator(summary, max_length=600)[0]['translation_text']

            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(translated)

            print(f"Processed: {filename} -> Saved to Drive")

        except Exception as e:
            print(f"Error processing {filename}: {str(e)}")
            continue

process_files()
print(f"\nAll cleaned files saved to: {OUTPUT_DIR}")