{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlOiYUukOJHI"
      },
      "outputs": [],
      "source": [
        "# from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# load_dotenv()\n",
        "# huggingface_token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
        "# openai_token = os.getenv(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETSxI1zVOJHK"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Fix common OCR patterns\n",
        "    text = re.sub(r'[«»\"“”‘’]', '', text)  # Remove quotation marks\n",
        "    text = re.sub(r'\\s+', ' ', text)       # Normalize spaces\n",
        "    text = re.sub(r'[^a-zA-ZáéíóúñÁÉÍÓÚÑ0-9\\s.,;:¿?¡!\\-]', '', text)  # Remove special chars\n",
        "    return text.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBRvmh_GP28j"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "API_URL = \"https://api-inference.huggingface.co/models/mistralai/Mistral-7B-v0.1\"\n",
        "headers = {\"Authorization\": f\"Bearer {huggingface_token}\"}\n",
        "\n",
        "def fix_with_huggingface(text):\n",
        "    payload = {\n",
        "        \"inputs\": f\"Corrige los errores de OCR en este texto en español: {text}\",\n",
        "        \"parameters\": {\"max_length\": 500}\n",
        "    }\n",
        "    response = requests.post(API_URL, headers=headers, json=payload)\n",
        "    return response.json()[0]['generated_text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edx2Yr3IOJHK"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load Mistral-7B for text correction\n",
        "fixer = pipeline(\"text-generation\", model=\"mistralai/Mistral-7B-v0.1\", token = huggingface_token)\n",
        "\n",
        "def fix_with_mistral(text):\n",
        "    prompt = f\"Corrige los errores de OCR en este texto en español: {text}\\nTexto corregido:\"\n",
        "    corrected = fixer(prompt, max_length=100, num_return_sequences=1)\n",
        "    return corrected[0]['generated_text'].split(\"Texto corregido:\")[-1].strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODw8dt_5OJHL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "def clean_file(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "    cleaned = preprocess_text(text)\n",
        "    corrected = fix_with_huggingface(cleaned)\n",
        "    with open(f\"cleaned_{file_path}\", 'w', encoding='utf-8') as f:\n",
        "        f.write(corrected)\n",
        "\n",
        "def batch_clean(files, batch_size=100):\n",
        "    for i in range(0, len(files), batch_size):\n",
        "        batch = files[i:i+batch_size]\n",
        "        with ThreadPoolExecutor() as executor:\n",
        "            executor.map(clean_file, batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HLgwbdCQRgo"
      },
      "outputs": [],
      "source": [
        "file_path = \"19200117 english folder.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jM48mv9jQZxG"
      },
      "outputs": [],
      "source": [
        "with open(file_path, 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "cleaned = preprocess_text(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pOJY6nugVT5F"
      },
      "outputs": [],
      "source": [
        "cleaned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Do2HlpcfTQ-9"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\", token=huggingface_token)\n",
        "\n",
        "prompt =  f\"Corrige los errores de OCR en este texto en español: {cleaned}\"\n",
        "\n",
        "model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "model.to(device)\n",
        "\n",
        "generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n",
        "tokenizer.batch_decode(generated_ids)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oyh3lNLiaAfs"
      },
      "outputs": [],
      "source": [
        "!pip install pyspellchecker\n",
        "!pip install pyenchant\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IESHdmDtZwN6"
      },
      "outputs": [],
      "source": [
        "from spellchecker import SpellChecker\n",
        "\n",
        "def correct_spelling(text):\n",
        "    spell = SpellChecker()\n",
        "    words = text.split()\n",
        "    corrected_text = []\n",
        "\n",
        "    for word in words:\n",
        "        corrected_word = spell.correction(word)\n",
        "        corrected_text.append(corrected_word)\n",
        "\n",
        "    return \" \".join(corrected_text)\n",
        "\n",
        "ocr_text = \"Ths is smpl ocr txt with errrs\"\n",
        "corrected_text = correct_spelling(ocr_text)\n",
        "print(corrected_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTvNcP1_Z0f7"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "def correct_with_language_model(text):\n",
        "    nlp = pipeline(\"fill-mask\", model=\"bert-base-uncased\", token = huggingface_token)\n",
        "    tokens = text.split()\n",
        "    corrected_tokens = []\n",
        "\n",
        "    for i, token in enumerate(tokens):\n",
        "        if token in [',', '.', '?', '!', ':', ';']:\n",
        "            corrected_tokens.append(token)\n",
        "            continue\n",
        "\n",
        "        masked_text = \" \".join(tokens[:i] + [\"[MASK]\"] + tokens[i+1:])\n",
        "        corrected_word = nlp(masked_text)[0]['token_str']\n",
        "        corrected_tokens.append(corrected_word)\n",
        "\n",
        "    return \" \".join(corrected_tokens)\n",
        "\n",
        "ocr_text = \"Ths is a smple exmple of ocr txt\"\n",
        "corrected_text = correct_with_language_model(ocr_text)\n",
        "print(corrected_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zyeiug1bNYh"
      },
      "outputs": [],
      "source": [
        "ocr_text = \"Ths is a smple exmple of ocr txt\"\n",
        "ocr_text = correct\n",
        "corrected_text = correct_with_language_model(ocr_text)\n",
        "print(corrected_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gC_rBoBwcfEB"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForMaskedLM, AutoTokenizer, pipeline\n",
        "\n",
        "# Load the model and tokenizer using AutoModelForMaskedLM to avoid warnings\n",
        "model_name = \"bert-base-uncased\"\n",
        "\n",
        "# Initialize the pipeline for fill-mask with trust_remote_code=True to suppress future warnings\n",
        "nlp = pipeline(\n",
        "    \"fill-mask\",\n",
        "    model=AutoModelForMaskedLM.from_pretrained(model_name, trust_remote_code=True),\n",
        "    tokenizer=AutoTokenizer.from_pretrained(model_name)\n",
        ")\n",
        "\n",
        "def correct_ocr_text(text):\n",
        "    corrected_text = []\n",
        "    words = text.split()\n",
        "\n",
        "    for word in words:\n",
        "        # Only apply MLM on words that are reasonably long to avoid unnecessary predictions\n",
        "        if len(word) > 3:\n",
        "            masked_word = word[:len(word)//2] + \"[MASK]\" + word[len(word)//2+1:]\n",
        "            try:\n",
        "                result = nlp(masked_word)\n",
        "                corrected_word = result[0][\"sequence\"].replace(\"[SEP]\", \"\").replace(\"[CLS]\", \"\")\n",
        "                corrected_text.append(corrected_word.strip())\n",
        "            except Exception as e:\n",
        "                corrected_text.append(word)  # Fallback to the original word if something fails\n",
        "        else:\n",
        "            corrected_text.append(word)\n",
        "\n",
        "    return \" \".join(corrected_text)\n",
        "\n",
        "# Example usage\n",
        "ocr_text = \"it is a component implementation ##t the .\"\n",
        "corrected_text = correct_ocr_text(ocr_text)\n",
        "print(corrected_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eH3O-Ny0dCGA"
      },
      "outputs": [],
      "source": [
        "# Example usage\n",
        "ocr_text = \"Ths is a smple exmple of ocr txt\"\n",
        "corrected_text = correct_ocr_text(ocr_text)\n",
        "print(corrected_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVVxBhmBfbJl"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load a text-generation pipeline\n",
        "generator = pipeline(\"text-generation\", model=\"gpt2\", token = huggingface_token)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPVQ5GWbfvaB"
      },
      "outputs": [],
      "source": [
        "def fix_ocr_text_with_generation(text):\n",
        "    # Generate the corrected text from the garbled input\n",
        "    generated_text = generator(text, max_length=1000, num_return_sequences=1)\n",
        "    return generated_text[0]['generated_text']\n",
        "\n",
        "# Example usage\n",
        "ocr_text = \"Fix this weird text i got from an OCR: `it is a comp. nent impleme de tation ##t the .`\"\n",
        "corrected_text = fix_ocr_text_with_generation(ocr_text)\n",
        "print(corrected_text)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
